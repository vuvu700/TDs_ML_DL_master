{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP N°5: Ensemebles de modèles\n",
    "\n",
    "Dans ce TP nous allons voir comment combiner des modèles pour améliorer les performances de prédiction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A compléter ici :\n",
    "### Nom : Andrieu\n",
    "### Prénom : Ludovic\n",
    "### N étudiant : 22103219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Le **Bagging** (Bootstrap Aggregating) est une méthode d'**apprentissage d'ensemble** qui vise à améliorer la précision et la robustesse des modèles en combinant plusieurs estimateurs entraînés indépendamment. \n",
    "\n",
    "### Principe du Bagging\n",
    "\n",
    "Le Bagging repose sur les étapes suivantes :\n",
    "\n",
    "1. **Génération de plusieurs échantillons bootstrap**  \n",
    "   - À partir du jeu de données d'entraînement, on génère plusieurs sous-échantillons en tirant aléatoirement avec remise.\n",
    "   \n",
    "2. **Entraînement de plusieurs modèles**  \n",
    "   - Un même modèle de base (ex. : arbre de décision, régression) est entraîné sur chaque sous-échantillon.\n",
    "\n",
    "3. **Agrégation des prédictions**  \n",
    "   - Pour une tâche de classification : on effectue un **vote majoritaire** entre les modèles.  \n",
    "   - Pour une tâche de régression : on prend la **moyenne** des prédictions.\n",
    "\n",
    "### Avantages du Bagging\n",
    "\n",
    "- **Réduction de la variance** : en combinant plusieurs modèles, le Bagging permet de stabiliser les prédictions et d'éviter le surapprentissage.  \n",
    "- **Robustesse aux perturbations** : les modèles individuels étant entraînés sur des échantillons différents, ils sont moins sensibles aux variations du jeu de données.  \n",
    "- **Parallélisation possible** : chaque modèle étant entraîné indépendamment, le Bagging est facilement parallélisable.  \n",
    "\n",
    "Une application courante du Bagging est l'algorithme **Random Forest**, qui utilise des arbres de décision comme modèles de base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1.1 : Implémentation manuelle du Bagging en régression\n",
    "\n",
    "Dans cet exercice, nous allons implémenter **manuellement** l’algorithme de **Bagging** pour un problème de régression, sans utiliser `BaggingRegressor` de `scikit-learn`. Nous utiliserons un **arbre de décision** comme modèle de base.\n",
    "\n",
    "## Objectifs de l'exercice\n",
    "\n",
    "- Comprendre le fonctionnement interne du Bagging appliqué à la régression.  \n",
    "- Générer des échantillons bootstrap à partir des données d'entraînement.  \n",
    "- Entraîner plusieurs modèles indépendants sur ces échantillons.  \n",
    "- Agréger les prédictions des modèles en utilisant la **moyenne** des prédictions.  \n",
    "- Comparer les performances du modèle simple et du modèle avec Bagging.  \n",
    "\n",
    "## Consignes\n",
    "\n",
    "1. **Chargement des données**  \n",
    "   - Générer un jeu de données de régression (`make_regression` de `sklearn.datasets`).  \n",
    "   - Séparer les données en un ensemble d’entraînement et un ensemble de test.  \n",
    "\n",
    "2. **Entraînement d'un modèle de base**  \n",
    "   - Entraîner un **arbre de décision** (`DecisionTreeRegressor`) sur les données d'entraînement.  \n",
    "   - Évaluer sa performance sur les données de test avec l'**erreur quadratique moyenne (MSE)**.  \n",
    "\n",
    "3. **Implémentation manuelle du Bagging**  \n",
    "   - **Génération des échantillons bootstrap** : créer plusieurs sous-échantillons en tirant aléatoirement avec remise.  \n",
    "   - **Entraînement de plusieurs modèles** : entraîner un **arbre de décision** sur chaque sous-échantillon.  \n",
    "   - **Agrégation des prédictions** : prendre la **moyenne des prédictions** des modèles entraînés.  \n",
    "\n",
    "4. **Comparaison des résultats**  \n",
    "   - Comparer la performance du modèle unique et du modèle avec Bagging.  \n",
    "   - Comparer la performance en fonction de nombre de sous-modèles (en utilisant matplotlib).\n",
    "\n",
    "## Indications\n",
    "\n",
    "- Utiliser `train_test_split` de `sklearn.model_selection` pour diviser les données.  \n",
    "- Pour créer les échantillons bootstrap, utiliser `np.random.choice` avec `replace=True`.  \n",
    "- Pour évaluer les modèles, utiliser `mean_squared_error` où `mean_absolute_error` de `sklearn.metrics`.  \n",
    "- Tester différents nombres d’estimateurs (`n_estimators`) pour observer l’impact du Bagging.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X, y, *_ = make_regression(n_samples=1000, n_features=20, n_informative=10, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyBaggingRegressor():\n",
    "    def __init__(self, model:DecisionTreeRegressor, nbSubModels:int) -> None:\n",
    "        assert model.get_params(deep=False)[\"criterion\"] in (\"squared_error\", \"absolute_error\", )\n",
    "        self.allModels: list[DecisionTreeRegressor] = [\n",
    "            DecisionTreeRegressor().set_params(**model.get_params()) \n",
    "            for _ in range(nbSubModels)]\n",
    "    @property\n",
    "    def nbSubModels(self)->int: \n",
    "        return len(self.allModels)\n",
    "    \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray)->None:\n",
    "        for modelIndex, model in enumerate(self.allModels):\n",
    "            model.fit(*self._createSubSamples(\n",
    "                self._preprocessDatas(X, modelIndex), y))\n",
    "\n",
    "    def predict(self, X:np.ndarray)->np.ndarray:\n",
    "        y_sum = np.zeros((len(X), ))\n",
    "        for modelIndex in range(self.nbSubModels):\n",
    "            y_sum += self.predict_model(X, modelIndex=modelIndex)\n",
    "        return (y_sum / self.nbSubModels)\n",
    "    \n",
    "    def predict_model(self, X:np.ndarray, modelIndex:int)->np.ndarray:\n",
    "        return self.allModels[modelIndex].predict(self._preprocessDatas(X, modelIndex))\n",
    "\n",
    "    def _preprocessDatas(self, X:np.ndarray, modelIndex:int)->np.ndarray:\n",
    "        return X\n",
    "\n",
    "    def _createSubSamples(self, X:np.ndarray, y:np.ndarray)->tuple[np.ndarray, np.ndarray]:\n",
    "        N = len(y)\n",
    "        subIndexes = np.random.randint(0, N, size=(N, ))\n",
    "        return (X[subIndexes], y[subIndexes])\n",
    "\n",
    "\n",
    "    def evaluate(self, X:np.ndarray, y_true:np.ndarray)->float:\n",
    "        loss = mean_squared_error(y_true, self.predict(X))\n",
    "        return float(loss) # should be from a np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 1196.347\n",
      "test loss = 7322.064\n"
     ]
    }
   ],
   "source": [
    "myBagg = MyBaggingRegressor(model=DecisionTreeRegressor(), nbSubModels=100)\n",
    "myBagg.fit(X_train, y_train)\n",
    "print(f\"train loss = {myBagg.evaluate(X_train, y_train):.3f}\")\n",
    "print(f\"test loss = {myBagg.evaluate(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2\n",
    "\n",
    "- Faire la meme chose en utilisant `BaggingRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 1173.439\n",
      "test loss = 7336.207\n",
      "\n",
      "base mse: 20083.367 +/- 1839.782\n",
      "bagg mse: 8354.688 +/- 1069.676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "baseModel = DecisionTreeRegressor()\n",
    "baggModel = BaggingRegressor(baseModel, n_estimators=100, n_jobs=5)\n",
    "\n",
    "baggModel.fit(X_train, y_train)\n",
    "y_train_pred = baggModel.predict(X_train)\n",
    "y_test_pred = baggModel.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "base_cv_scores = -1 * cross_val_score(baseModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "bagg_cv_scores = -1 * cross_val_score(baggModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"base mse: {base_cv_scores.mean():.3f} +/- {base_cv_scores.std():.3f}\")\n",
    "print(f\"bagg mse: {bagg_cv_scores.mean():.3f} +/- {bagg_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 : Implémentation des forêts aléatoires\n",
    "\n",
    "Dans cet exercice, nous allons explorer **les forêts aléatoires**, une amélioration du Bagging qui introduit une **sélection aléatoire des caractéristiques** à chaque division de l’arbre. Nous commencerons par une **implémentation manuelle**, avant d'utiliser `RandomForestRegressor` de `scikit-learn`.\n",
    "\n",
    "## Objectifs de l'exercice\n",
    "\n",
    "- Comprendre la différence entre le Bagging et les forêts aléatoires.  \n",
    "- Implémenter une forêt aléatoire manuellement en modifiant le Bagging.  \n",
    "- Observer l'impact de la sélection aléatoire des caractéristiques sur la performance du modèle.  \n",
    "- Comparer les résultats entre l'implémentation manuelle et `RandomForestRegressor`.\n",
    "\n",
    "## Exercice 2.1 : Implémentation manuelle des forêts aléatoires\n",
    "\n",
    "### Consignes\n",
    "\n",
    "1. **Chargement des données**  \n",
    "   - Utiliser `make_regression` de `sklearn.datasets` pour générer un problème de régression.  \n",
    "   - Séparer les données en un ensemble d’entraînement et un ensemble de test.  \n",
    "\n",
    "2. **Implémentation manuelle des forêts aléatoires**  \n",
    "   - Reprendre le code du **Bagging manuel** de l'exercice précédent.  \n",
    "   - Modifier l’entraînement des modèles pour que chaque **arbre de décision sélectionne un sous-ensemble aléatoire de features** à chaque division. Pour les selectionner, vous \n",
    "   - Tester différents nombres de variables sélectionnées (`max_features`).  \n",
    "\n",
    "3. **Évaluation et comparaison**  \n",
    "   - Comparer la performance du modèle avec et sans sélection aléatoire des features. \n",
    "\n",
    "### Indications\n",
    "\n",
    "- Pour choisir les features à utiliser pour chaque modèle vous pouver utiliser `np.random.choice` sur `X_train.shape[1]`\n",
    "- Tester différentes valeurs de `max_features` (ex. : `sqrt(n_features)`, `log2(n_features)`, ou un nombre fixe).  \n",
    "- Évaluer la performance avec `mean_squared_error`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from typing_extensions import Literal, override\n",
    "\n",
    "_MaxFeaturesKind = Literal[\"sqrt\", \"log2\"]\n",
    "\n",
    "class MyRandomForestRegressor(MyBaggingRegressor):\n",
    "    def __init__(self, model: DecisionTreeRegressor, nbSubModels: int,\n",
    "                 datasNbFeatures:int, maxFeatures:int|_MaxFeaturesKind, minFeatures:int) -> None:\n",
    "        assert minFeatures >= 1, f\"can't train a model on {minFeatures} features\"\n",
    "        super().__init__(model, nbSubModels)\n",
    "        self.datasNbFeatures: int = datasNbFeatures\n",
    "        self.maxFeatures:int|_MaxFeaturesKind = maxFeatures\n",
    "        self.minFeatures:int = minFeatures\n",
    "        self.perModelsFeatures: list[np.ndarray] = []\n",
    "        for index in range(self.nbSubModels):\n",
    "            self.perModelsFeatures.append(self._decideSubFeatures())\n",
    "    \n",
    "    def _getMaxFeatures(self, nbFeatures: int)->int:\n",
    "        maxFeatures: int\n",
    "        if self.maxFeatures == \"sqrt\":\n",
    "            maxFeatures = int(np.ceil(np.sqrt(nbFeatures)))\n",
    "        elif self.maxFeatures == \"log2\":\n",
    "            maxFeatures = int(np.ceil(np.log2(nbFeatures)))\n",
    "        else: # => self.maxFeatures is an int\n",
    "            maxFeatures = int(self.maxFeatures)\n",
    "        assert maxFeatures >= 1, f\"can't train a model on {maxFeatures} features\"\n",
    "        assert maxFeatures <= maxFeatures, f\"the datas dont have enougth features ({maxFeatures} > {nbFeatures})\"\n",
    "        return maxFeatures\n",
    "    \n",
    "    def _decideSubFeatures(self):\n",
    "        nbSubFeatures: int = np.random.randint(\n",
    "            self.minFeatures, self._getMaxFeatures(self.datasNbFeatures)+1)\n",
    "        nbFeatures: int = X.shape[1]\n",
    "        subFeaturesIndexes = np.arange(nbFeatures)\n",
    "        np.random.shuffle(subFeaturesIndexes)\n",
    "        subFeaturesIndexes = subFeaturesIndexes[: nbSubFeatures]\n",
    "        return subFeaturesIndexes\n",
    "    \n",
    "    @override\n",
    "    def _preprocessDatas(self, X:np.ndarray, modelIndex:int)->np.ndarray:\n",
    "        return X[:, self.perModelsFeatures[modelIndex]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, *_ = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=10, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn model\n",
      "train loss = 1528.968\n",
      "test loss = 10329.343\n",
      "\n",
      "my model\n",
      "train loss = 1140.441\n",
      "test loss = 7456.646\n"
     ]
    }
   ],
   "source": [
    "myRandForest = MyRandomForestRegressor(\n",
    "    model=DecisionTreeRegressor(), nbSubModels=100,\n",
    "    datasNbFeatures=(X.shape[1]), maxFeatures=15, minFeatures=10)\n",
    "myRandForest.fit(X_train, y_train)\n",
    "print(\"sklearn model\")\n",
    "print(f\"train loss = {myRandForest.evaluate(X_train, y_train):.3f}\")\n",
    "print(f\"test loss = {myRandForest.evaluate(X_test, y_test):.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"my model\")\n",
    "myBagg = MyBaggingRegressor(model=DecisionTreeRegressor(), nbSubModels=100)\n",
    "myBagg.fit(X_train, y_train)\n",
    "print(f\"train loss = {myBagg.evaluate(X_train, y_train):.3f}\")\n",
    "print(f\"test loss = {myBagg.evaluate(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2.2 : Utilisation de `RandomForestRegressor`\n",
    "\n",
    "### Consignes\n",
    "\n",
    "1. Utiliser `RandomForestRegressor` de `sklearn.ensemble` avec les mêmes données.  \n",
    "2. Ajuster les paramètres `n_estimators` et `max_features`.  \n",
    "3. Comparer les performances entre l’implémentation manuelle et `RandomForestRegressor`.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 1171.860\n",
      "test loss = 7303.203\n",
      "\n",
      "base mse: 20257.571 +/- 2217.437\n",
      "bagg mse: 8427.153 +/- 1061.554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "baseModel = DecisionTreeRegressor()\n",
    "randForest = RandomForestRegressor(100, max_features=15)\n",
    "\n",
    "randForest.fit(X_train, y_train)\n",
    "y_train_pred = randForest.predict(X_train)\n",
    "y_test_pred = randForest.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "base_cv_scores = -1 * cross_val_score(baseModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "forest_cv_scores = -1 * cross_val_score(randForest, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"base mse: {base_cv_scores.mean():.3f} +/- {base_cv_scores.std():.3f}\")\n",
    "print(f\"bagg mse: {forest_cv_scores.mean():.3f} +/- {forest_cv_scores.std():.3f}\")\n",
    "\n",
    "# les performance de MyRandomForestRegressor sont bien en dessous de celles de sklearn\n",
    "# probablement duent a la selection des features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3.1 : AdaBoost\n",
    "\n",
    "L'**AdaBoost** (Adaptive Boosting) est une méthode d'ensemble qui combine plusieurs modèles faibles (souvent des arbres de décision peu profonds) pour créer un modèle puissant. Le principe de base est de **donner plus de poids aux erreurs** faites par les premiers modèles et de corriger ces erreurs dans les modèles suivants. Chaque modèle successif est entraîné sur les exemples mal classés du modèle précédent.\n",
    "\n",
    "## Objectifs de l'exercice\n",
    "\n",
    "1. Entraîner un modèle AdaBoost en régression avec `AdaBoostRegressor`.\n",
    "2. Comparer les performances d'un **modèle de base** (par exemple, un arbre de décision) et du modèle **AdaBoost**.\n",
    "\n",
    "### Consignes\n",
    "\n",
    "1. **Chargement des données**\n",
    "   - Utilisez `make_regression` pour générer un jeu de données de régression.\n",
    "   - Divisez les données en ensemble d’entraînement et ensemble de test.\n",
    "\n",
    "2. **Modèle de base : Arbre de décision**\n",
    "   - Entraînez un modèle de base (par exemple un arbre de décision) et évaluez sa performance sur les données de test avec **l'erreur quadratique moyenne (MSE)**.\n",
    "\n",
    "3. **Utilisation d'AdaBoostRegressor**\n",
    "   - Utilisez `AdaBoostRegressor` de `scikit-learn` pour entraîner un modèle AdaBoost sur les données.\n",
    "   - Comparez les performances du modèle AdaBoost avec celles du modèle de base.\n",
    "\n",
    "4. **Analyse des résultats**\n",
    "   - Que se passe-t-il si vous changez la profondeur maximale d'arbres utilisés?\n",
    "   - Que se passe-t-il si vous changez le nombre d'estimateurs dans `AdaBoostRegressor` ?\n",
    "\n",
    "### Indications\n",
    "\n",
    "- Utilisez `AdaBoostRegressor` de `sklearn.ensemble`.\n",
    "- Pour comparer les performances, utilisez `mean_squared_error` de `sklearn.metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.318\n",
      "test loss = 6411.306\n",
      "\n",
      "base mse: 20463.418 +/- 2700.162\n",
      "ada mse: 7573.067 +/- 1050.120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "baseModel_ada = DecisionTreeRegressor()\n",
    "adaModel = AdaBoostRegressor(baseModel_ada, n_estimators=100)\n",
    "\n",
    "adaModel.fit(X_train, y_train)\n",
    "y_train_pred = adaModel.predict(X_train)\n",
    "y_test_pred = adaModel.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "base_cv_scores = -1 * cross_val_score(baseModel_ada, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "ada_cv_scores = -1 * cross_val_score(adaModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"base mse: {base_cv_scores.mean():.3f} +/- {base_cv_scores.std():.3f}\")\n",
    "print(f\"ada mse: {ada_cv_scores.mean():.3f} +/- {ada_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3.2 : Gradient Boosting\n",
    "\n",
    "Le **Gradient Boosting** est une autre méthode d'ensemble basée sur l'optimisation d'un modèle prédictif, mais contrairement à l'AdaBoost, il ajuste les erreurs par **gradient de la fonction de coût**. Chaque modèle successif essaie de corriger l’erreur du modèle précédent, en se concentrant sur les résidus. Le Gradient Boosting est particulièrement puissant et largement utilisé dans des tâches complexes comme la régression et la classification.\n",
    "\n",
    "## Objectifs de l'exercice\n",
    "\n",
    "1. Entraîner un modèle de **Gradient Boosting** avec `GradientBoostingRegressor`.\n",
    "2. Comparer les performances du **modèle de base** (arbre de décision) et du modèle **Gradient Boosting**.\n",
    "\n",
    "### Consignes\n",
    "\n",
    "1. **Chargement des données**\n",
    "   - Utilisez `make_regression` pour générer un jeu de données de régression.\n",
    "   - Divisez les données en un ensemble d’entraînement et un ensemble de test.\n",
    "\n",
    "2. **Modèle de base : Arbre de décision**\n",
    "   - Entraînez un modèle de base (par exemple, un arbre de décision) et évaluez sa performance sur les données de test avec **l'erreur quadratique moyenne (MSE)**.\n",
    "\n",
    "3. **Utilisation de GradientBoostingRegressor**\n",
    "   - Utilisez `GradientBoostingRegressor` de `scikit-learn` pour entraîner un modèle Gradient Boosting sur les données.\n",
    "   - Comparez les performances du modèle Gradient Boosting avec celles du modèle de base.\n",
    "\n",
    "4. **Analyse des résultats**\n",
    "   - Que se passe-t-il si vous changez la profondeur maximale d'arbres utilisés? Visualisez la dépendance.\n",
    "   - Que se passe-t-il si vous changez le nombre d'arbres (`n_estimators`) dans `GradientBoostingRegressor` ?\n",
    "   - Comparez la vitesse de convergence d'erreur entre AdaBoost et Gradient boosting (visuellement).\n",
    "\n",
    "### Indications\n",
    "\n",
    "- Utilisez `GradientBoostingRegressor` de `sklearn.ensemble`.\n",
    "- Pour comparer les performances, utilisez `mean_squared_error` de `sklearn.metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 453.697\n",
      "test loss = 3223.681\n",
      "\n",
      "ada mse: 7459.951 +/- 1051.675\n",
      "GB mse: 3486.111 +/- 468.653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "GBModel = GradientBoostingRegressor(n_estimators=100, max_depth=3)\n",
    "\n",
    "GBModel.fit(X_train, y_train)\n",
    "y_train_pred = GBModel.predict(X_train)\n",
    "y_test_pred = GBModel.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "ada_cv_scores = -1 * cross_val_score(adaModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "gb_cv_scores = -1 * cross_val_score(GBModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"ada mse: {ada_cv_scores.mean():.3f} +/- {ada_cv_scores.std():.3f}\")\n",
    "print(f\"GB mse: {gb_cv_scores.mean():.3f} +/- {gb_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7b6defcf90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGdCAYAAAAbudkLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABlWElEQVR4nO3deXhU1cE/8O/s2Scbk8lACGFfAggB2VxQIYAsYn2LFU3xV4pVWUShttq31dq34o6t1KVqtVU01CJWBCOIgkYISyAKhFUCCVlIyDKTdWYyc35/3OSGYc0yM3cSvp/nuc8k956Ze+YmZL6cexaVEEKAiIiIqAtSK10BIiIiIl9h0CEiIqIui0GHiIiIuiwGHSIiIuqyGHSIiIioy2LQISIioi6LQYeIiIi6LAYdIiIi6rK0SldASW63G0VFRQgPD4dKpVK6OkRERNQKQghUV1fDYrFArb58m81VHXSKioqQkJCgdDWIiIioHQoKCtCjR4/Llrmqg054eDgA6UJFREQoXBsiIiJqDZvNhoSEBPlz/HKu6qDTfLsqIiKCQYeIiKiTaU23E3ZGJiIioi6LQYeIiIi6LAYdIiIi6rKu6j46RER09RFCoLGxES6XS+mq0CVoNBpotVqvTP3CoENERFcNh8OB4uJi1NXVKV0VuoKQkBDEx8dDr9d36HUYdIiI6KrgdruRl5cHjUYDi8UCvV7PyWIDkBACDocDZWVlyMvLQ79+/a44KeCVXrDVXn31VTF06FARHh4uwsPDxdixY8XGjRvl4/PmzRMAPLYxY8Z4vEZDQ4NYtGiRiImJESEhIWLmzJmioKDAo0xFRYW45557REREhIiIiBD33HOPqKys9Chz6tQpMWPGDBESEiJiYmLE4sWLhd1ub8vbEVarVQAQVqu1Tc8jIqLOp76+XuTm5ora2lqlq0KtUFtbK3Jzc0V9ff0Fx9ry+d2miNSjRw8888wz2LNnD/bs2YObb74Zt912Gw4ePCiXmTp1KoqLi+Vt48aNHq+xdOlSrFu3Dunp6cjMzERNTQ1mzJjhca907ty5yMnJQUZGBjIyMpCTk4O0tDT5uMvlwvTp01FbW4vMzEykp6dj7dq1WLZsWduTHhERXVU61DpAfuO1n1NHE1dUVJR46623hBBSi85tt912ybJVVVVCp9OJ9PR0eV9hYaFQq9UiIyNDCCFEbm6uACCysrLkMjt27BAAxOHDh4UQQmzcuFGo1WpRWFgol/nwww+FwWBoU+sMW3SIiK4ezS06F2shoMBzuZ+Xz1p0zuVyuZCeno7a2lqMGzdO3r9161aYTCb0798fCxYsQGlpqXwsOzsbTqcTqamp8j6LxYLk5GRs374dALBjxw4YjUaMGTNGLjN27FgYjUaPMsnJybBYLHKZKVOmwG63Izs7u71viYiIiLqYNged/fv3IywsDAaDAffffz/WrVuHwYMHAwCmTZuG1atX46uvvsKLL76I3bt34+abb4bdbgcAlJSUQK/XIyoqyuM14+LiUFJSIpcxmUwXnNdkMnmUiYuL8zgeFRUFvV4vl7kYu90Om83msREREXV2vXr1wssvv9zu56tUKnzyySeXPL5161aoVCpUVVW1+xxKaXPQGTBgAHJycpCVlYUHHngA8+bNQ25uLgDgzjvvxPTp05GcnIyZM2fi888/x9GjR7Fhw4bLvqYQwqPn+8V6wbenzPlWrFgBo9Eob1y5nIiI6MrGjx+P4uJiGI3GK5YNtFDU5qCj1+vRt29fjBo1CitWrMDw4cPxl7/85aJl4+PjkZiYiGPHjgEAzGYzHA4HKisrPcqVlpbKLTRmsxlnzpy54LXKyso8ypzfclNZWQmn03lBS8+5HnvsMVitVnkrKCho/Rtvg+xTFXhqfS7W7M73yesTERH5k16vh9ls7pTD8TvcpVkIId+aOl95eTkKCgoQHx8PAEhJSYFOp8PmzZvlMsXFxThw4ADGjx8PABg3bhysVit27doll9m5cyesVqtHmQMHDqC4uFgus2nTJhgMBqSkpFyyrgaDQV6p3JcrlucWV+Mf3+Vh08ELAxsREQUGIQTqHI2KbEKINtW1uroad999N0JDQxEfH4+VK1di4sSJWLp0qUeZuXPnIiwsDBaLBa+88kqbznH27FncfvvtCAkJQb9+/fDpp5/Kx85vpTl16hRmzpyJqKgohIaGYsiQIdi4cSNOnjyJm266CYDUpUSlUuHee+9tUz28rU0TBj7++OOYNm0aEhISUF1djfT0dGzduhUZGRmoqanBk08+iTvuuAPx8fE4efIkHn/8ccTGxuL2228HABiNRsyfPx/Lli1DTEwMoqOjsXz5cgwdOhSTJk0CAAwaNAhTp07FggUL8MYbbwAA7rvvPsyYMQMDBgwAAKSmpmLw4MFIS0vD888/j4qKCixfvhwLFizwWXhpiz7dQgEAP5bVKFwTIiK6lHqnC4P/8IUi5859agpC9K3/CH7kkUfw3Xff4dNPP0VcXBz+8Ic/YO/evbjmmmvkMs8//zwef/xxPPnkk/jiiy/w8MMPY+DAgZg8eXKrzvHHP/4Rzz33HJ5//nm88soruPvuu3Hq1ClER0dfUHbhwoVwOBz45ptvEBoaitzcXISFhSEhIQFr167FHXfcgSNHjiAiIgLBwcGtfp++0Kagc+bMGaSlpcn36YYNG4aMjAxMnjwZ9fX12L9/P/71r3+hqqoK8fHxuOmmm7BmzRqEh4fLr7Fy5UpotVrMmTMH9fX1uOWWW/Duu+9Co9HIZVavXo0lS5bIo7NmzZqFVatWycc1Gg02bNiABx98EBMmTEBwcDDmzp2LF154oaPXwyv6dAsDABRU1sPR6IZeyzkbiIiofaqrq/HPf/4TH3zwAW655RYAwDvvvOMx8hgAJkyYgN/+9rcAgP79++O7777DypUrWx107r33Xtx1110AgKeffhqvvPIKdu3ahalTp15QNj8/H3fccQeGDh0KAOjdu7d8rDkYmUwmREZGtu3N+kCbgs7bb799yWPBwcH44osrJ+OgoCC88sorl21Si46Oxvvvv3/Z1+nZsyc+++yzK55PCaZwA0L1GtQ6XMivqEVfU/iVn0RERH4VrNMg96kpip27tU6cOAGn04lrr71W3mc0GuW7HM3Oneql+fu2jMQaNmyY/HVoaCjCw8M9pog515IlS/DAAw9g06ZNmDRpEu644w6P5wcSNjX4gEqlQu+mVp0fy2oVrg0REV2MSqVCiF6ryNaWTr3N/XnOf05r+vm05Tw6ne6C57rd7ouW/eUvf4kTJ04gLS0N+/fvx6hRo9rcJ8hfGHR8hP10iIjIG/r06QOdTucxSMdms8kjmptlZWVd8P3AgQN9Vq+EhATcf//9+Pjjj7Fs2TK8+eabACCvNn7u0k5K4urlPtLconOCLTpERNQB4eHhmDdvHn79618jOjoaJpMJTzzxBNRqtUeLzXfffYfnnnsOs2fPxubNm/HRRx9dcR679lq6dCmmTZuG/v37o7KyEl999RUGDRoEAEhMTIRKpcJnn32GW2+9FcHBwQgLC/NJPVqDLTo+0rupRecEW3SIiKiDXnrpJYwbNw4zZszApEmTMGHCBAwaNAhBQUFymWXLliE7OxsjRozAn/70J7z44ouYMsU3fZBcLhcWLlwoj5QeMGAAXn31VQBA9+7d8cc//hG//e1vERcXh0WLFvmkDq2lEm0dzN+F2Gw2GI1GWK1Wrw9Lzy2y4da/fgtjsA45f5jcKSdZIiLqShoaGpCXl4ekpCSPgNAZ1dbWonv37njxxRcxf/58pavjE5f7ebXl85u3rnwkKTYUKhVgrXeiotaBmDCD0lUiIqJOat++fTh8+DCuvfZaWK1WPPXUUwCA2267TeGaBT7euvKRYL0GFqM0SdKJs+ynQ0REHfPCCy9g+PDhmDRpEmpra/Htt98iNjb2is9bvXo1wsLCLroNGTLEDzVXFlt0fKh3t1AUVtXjRFkNRve6cGZJIiKi1hgxYgSys7Pb9dxZs2ZhzJgxFz12/pDyrohBx4f6dAvDt8fOcuQVEREpJjw83GOFgqsNb135UMtcOgw6RERESmDQ8aGWuXQ4xJyIiEgJDDo+1DyXTn5FHZyui0+jTURERL7DoOND5ogghOg1aHQL5FfUKV0dIiKiqw6Djg9Ji3s2z5DMfjpERET+xqDjY71jm1cxZz8dIiLyj5MnT0KlUiEnJ8crr3fvvfdi9uzZly3Tq1cvvPzyy145nzcx6PgY17wiIqKrwe7du3Hfffe1qqw/QxHn0fGxPlzFnIiIrgLdunVTugoXxRYdH5NbdLgMBBFRYBECcNQqs7VxPe2MjAxcd911iIyMRExMDGbMmIEff/xRPr5r1y6MGDECQUFBGDVqFPbt2+fxfJfLhfnz5yMpKQnBwcEYMGAA/vKXv7T5kr3wwguIj49HTEwMFi5cCKfTKR87v5XmySefRM+ePWEwGGCxWLBkyRIAwMSJE3Hq1Ck8/PDDUKlUPl/0mi06PpYUKwWdiloHKmsdiArVK1wjIiICADjrgKctypz78SJAH9rq4rW1tXjkkUcwdOhQ1NbW4g9/+ANuv/125OTkoL6+HjNmzMDNN9+M999/H3l5eXjooYc8nu92u9GjRw/8+9//RmxsLLZv34777rsP8fHxmDNnTqvq8PXXXyM+Ph5ff/01jh8/jjvvvBPXXHMNFixYcEHZ//znP1i5ciXS09MxZMgQlJSU4PvvvwcAfPzxxxg+fDjuu+++iz7X2xh0fCxEr4XFGIQiawNOnK1BSijXvCIiora54447PL5/++23YTKZkJubi+3bt8PlcuEf//gHQkJCMGTIEJw+fRoPPPCAXF6n0+GPf/yj/H1SUhK2b9+Of//7360OOlFRUVi1ahU0Gg0GDhyI6dOnY8uWLRcNK/n5+TCbzZg0aRJ0Oh169uyJa6+9FgAQHR0NjUaD8PBwmM3m9lyONmHQ8YM+pjAUWRvwY1ktUhIZdIiIAoIuRGpZUercbfDjjz/i97//PbKysnD27Fm43dIktPn5+Th06BCGDx+OkJCW1xw3btwFr/H666/jrbfewqlTp1BfXw+Hw4Frrrmm1XUYMmQINBqN/H18fDz2799/0bI//elP8fLLL6N3796YOnUqbr31VsycORNarf9jB/vo+EHvWM6lQ0QUcFQq6faRElsb+6XMnDkT5eXlePPNN7Fz507s3LkTAOBwOCBa0d/n3//+Nx5++GH84he/wKZNm5CTk4P/9//+HxwOR6vrcP5K5yqVSg5c50tISMCRI0fwt7/9DcHBwXjwwQdxww03ePTp8Re26PhB85pXnEuHiIjaqry8HIcOHcIbb7yB66+/HgCQmZkpHx88eDDee+891NfXIzg4GACQlZXl8Rrffvstxo8fjwcffFDed25nZl8IDg7GrFmzMGvWLCxcuBADBw7E/v37MXLkSOj1erhcLp+evxlbdPyAc+kQEVF7RUVFISYmBn//+99x/PhxfPXVV3jkkUfk43PnzoVarcb8+fORm5uLjRs34oUXXvB4jb59+2LPnj344osvcPToUfz+97/H7t27fVbnd999F2+//TYOHDiAEydO4L333kNwcDASExMBSCO0vvnmGxQWFuLs2bM+qwfAoOMXzXPp5FfUoZGLexIRURuo1Wqkp6cjOzsbycnJePjhh/H888/Lx8PCwrB+/Xrk5uZixIgR+N3vfodnn33W4zXuv/9+/OQnP8Gdd96JMWPGoLy83KN1x9siIyPx5ptvYsKECRg2bBi2bNmC9evXIyYmBgDw1FNP4eTJk+jTp4/P599Ridbc3OuibDYbjEYjrFYrIiIifHYet1tgyBNfoN7pwtfLJ8pDzomIyH8aGhqQl5eHpKQkBAUFKV0duoLL/bza8vnNFh0/UKtV6GOSws2RkmqFa0NERHT1YNDxk0FmKXEeKrYpXBMiIiJPYWFhl9y+/fZbpavXIRx15SeD4hl0iIgoMF1ulfPu3bv7ryI+wKDjJ3LQKWHQISKiwNK3b1+lq+AzvHXlJ4Obgk5BRT2qG/w/YRIREUmu4jE4nYq3fk4MOn5iDNHBYpR6jR9mh2QiIr9rntm3rq5O4ZpQazT/nM6fkbmteOvKjwbFR6DI2oBDxTaM7sU1r4iI/Emj0SAyMhKlpaUAgJCQEKjauBQD+Z4QAnV1dSgtLUVkZKTH+lrtwaDjR4PiI7DlcCk7JBMRKaR5tezmsEOBKzIy0iurmzPo+FFzh+TcIgYdIiIlqFQqxMfHw2QyKbLAJLWOTqfrcEtOMwYdPxpskYLOkTPVcLkFNGo2mRIRKUGj0Xjtg5QCGzsj+1FidAhC9Bo0ON3IO1urdHWIiIi6PAYdP1KrVRhgDgfAiQOJiIj8oU1B57XXXsOwYcMQERGBiIgIjBs3Dp9//rl8XAiBJ598EhaLBcHBwZg4cSIOHjzo8Rp2ux2LFy9GbGwsQkNDMWvWLJw+fdqjTGVlJdLS0mA0GmE0GpGWloaqqiqPMvn5+Zg5cyZCQ0MRGxuLJUuWwOFwtPHt+x9nSCYiIvKfNgWdHj164JlnnsGePXuwZ88e3HzzzbjtttvkMPPcc8/hpZdewqpVq7B7926YzWZMnjwZ1dUt88YsXboU69atQ3p6OjIzM1FTU4MZM2bA5XLJZebOnYucnBxkZGQgIyMDOTk5SEtLk4+7XC5Mnz4dtbW1yMzMRHp6OtauXYtly5Z19Hr4HIMOERGRH4kOioqKEm+99ZZwu93CbDaLZ555Rj7W0NAgjEajeP3114UQQlRVVQmdTifS09PlMoWFhUKtVouMjAwhhBC5ubkCgMjKypLL7NixQwAQhw8fFkIIsXHjRqFWq0VhYaFc5sMPPxQGg0FYrdZW191qtQoAbXpOR+05WS4Sf/OZGPPnL/12TiIioq6kLZ/f7e6j43K5kJ6ejtraWowbNw55eXkoKSlBamqqXMZgMODGG2/E9u3bAQDZ2dlwOp0eZSwWC5KTk+UyO3bsgNFoxJgxY+QyY8eOhdFo9CiTnJwMi8Uil5kyZQrsdjuys7Pb+5b8YkDTKuYltgZU1Ab+rTYiIqLOrM1BZ//+/QgLC4PBYMD999+PdevWYfDgwSgpKQEAxMXFeZSPi4uTj5WUlECv1yMqKuqyZUwm0wXnNZlMHmXOP09UVBT0er1c5mLsdjtsNpvH5m9hBi0SY0IA8PYVERGRr7U56AwYMAA5OTnIysrCAw88gHnz5iE3N1c+fv502kKIK06xfX6Zi5VvT5nzrVixQu7gbDQakZCQcNl6+cpg9tMhIiLyizYHHb1ej759+2LUqFFYsWIFhg8fjr/85S/yNM3nt6iUlpbKrS9msxkOhwOVlZWXLXPmzJkLzltWVuZR5vzzVFZWwul0XtDSc67HHnsMVqtV3goKCtr47r1DniGZQYeIiMinOjyPjhACdrsdSUlJMJvN2Lx5s3zM4XBg27ZtGD9+PAAgJSUFOp3Oo0xxcTEOHDgglxk3bhysVit27doll9m5cyesVqtHmQMHDqC4uFgus2nTJhgMBqSkpFyyrgaDQR4a37wpoWXkFVcxJyIi8qU2LQHx+OOPY9q0aUhISEB1dTXS09OxdetWZGRkQKVSYenSpXj66afRr18/9OvXD08//TRCQkIwd+5cAIDRaMT8+fOxbNkyxMTEIDo6GsuXL8fQoUMxadIkAMCgQYMwdepULFiwAG+88QYA4L777sOMGTMwYMAAAEBqaioGDx6MtLQ0PP/886ioqMDy5cuxYMECxcJLWwyKlyYNPF5aDUejG3ot520kIiLyhTYFnTNnziAtLQ3FxcUwGo0YNmwYMjIyMHnyZADAo48+ivr6ejz44IOorKzEmDFjsGnTJoSHh8uvsXLlSmi1WsyZMwf19fW45ZZb8O6773qsObJ69WosWbJEHp01a9YsrFq1Sj6u0WiwYcMGPPjgg5gwYQKCg4Mxd+5cvPDCCx26GP7SPTIYEUFa2Boa8WNZjdzCQ0RERN6lEkIIpSuhFJvNBqPRCKvV6veWoDlv7MCuvAq8+NPhuCOlh1/PTURE1Jm15fOb90wUwpFXREREvsegoxA56JQw6BAREfkKg45CBlukoHOg0Iar+O4hERGRTzHoKKR/XDj0GjWs9U4UVNQrXR0iIqIuiUFHIXqtGgObhpnvL7QqXBsiIqKuiUFHQUO7GwEw6BAREfkKg46CWoJOlbIVISIi6qIYdBSU3Bx0TlvZIZmIiMgHGHQU1D8uHHqtGraGRuRX1CldHSIioi6HQUdBeq0ag8zskExEROQrDDoKG9qDHZKJiIh8hUFHYUPP6adDRERE3sWgo7Ch3SMBSC067JBMRETkXQw6CusXFwa9Vo3qhkacKmeHZCIiIm9i0FGYTqPGoKYFPtlPh4iIyLsYdALAsKZ+OgcYdIiIiLyKQScANHdI/oEdkomIiLyKQScANA8xP1BkhdvNDslERETewqATAPqZwmBo7pDMGZKJiIi8hkEnAGjZIZmIiMgnGHQCxLAe7JBMRETkbQw6ASJZ7pBcpWxFiIiIuhAGnQDR3KJzsNDGDslERERewqATIPp2C0OQTo1qeyNOltcqXR0iIqIugUEnQJzbIZnz6RAREXkHg04AuSYhEgCQU1ClaD2IiIi6CgadADKyZxQAYG9+pcI1ISIi6hoYdALIiJ6RAIDcIhsanC5lK0NERNQFMOgEkO6RwTCFG9DoFpw4kIiIyAsYdAKISqVquX11ireviIiIOopBJ8A0377al1+laD2IiIi6AgadADMysaVDshCcOJCIiKgjGHQCzNDuRmjVKpRW21FkbVC6OkRERJ0ag06ACdJpMNgiTRzIfjpEREQdw6ATgEY0TRzI+XSIiIg6hkEnADX302GHZCIioo5h0AlAIxKkoHOwyMqJA4mIiDqAQScAJUQHIzZMD6dL4GCRTenqEBERdVptCjorVqzA6NGjER4eDpPJhNmzZ+PIkSMeZe69916oVCqPbezYsR5l7HY7Fi9ejNjYWISGhmLWrFk4ffq0R5nKykqkpaXBaDTCaDQiLS0NVVVVHmXy8/Mxc+ZMhIaGIjY2FkuWLIHD4WjLWwpIKpUK1yQ0375iPx0iIqL2alPQ2bZtGxYuXIisrCxs3rwZjY2NSE1NRW1trUe5qVOnori4WN42btzocXzp0qVYt24d0tPTkZmZiZqaGsyYMQMuV8ttmrlz5yInJwcZGRnIyMhATk4O0tLS5OMulwvTp09HbW0tMjMzkZ6ejrVr12LZsmXtuQ4BZ2RiJAB2SCYiIuoQ0QGlpaUCgNi2bZu8b968eeK222675HOqqqqETqcT6enp8r7CwkKhVqtFRkaGEEKI3NxcAUBkZWXJZXbs2CEAiMOHDwshhNi4caNQq9WisLBQLvPhhx8Kg8EgrFZrq+pvtVoFgFaXbzVHvRAf3y/E2ePtfokdP54Vib/5TIx9+ksvVoyIiKjza8vnd4f66Fit0sKT0dHRHvu3bt0Kk8mE/v37Y8GCBSgtLZWPZWdnw+l0IjU1Vd5nsViQnJyM7du3AwB27NgBo9GIMWPGyGXGjh0Lo9HoUSY5ORkWi0UuM2XKFNjtdmRnZ3fkbXXc5t8D338AvHMrUHq4XS8xrIcRGrUKxdYGFFvrvVxBIiKiq0O7g44QAo888giuu+46JCcny/unTZuG1atX46uvvsKLL76I3bt34+abb4bdbgcAlJSUQK/XIyoqyuP14uLiUFJSIpcxmUwXnNNkMnmUiYuL8zgeFRUFvV4vlzmf3W6HzWbz2Hzihl8DpsFATQnw7q1A8fdtfokQvRYDzeEAOMyciIiovdoddBYtWoQffvgBH374ocf+O++8E9OnT0dycjJmzpyJzz//HEePHsWGDRsu+3pCCKhUKvn7c7/uSJlzrVixQu7cbDQakZCQcNk6tVuYCbh3A2AZAdSVA+/OBAp2t/llmhf45AzJRERE7dOuoLN48WJ8+umn+Prrr9GjR4/Llo2Pj0diYiKOHTsGADCbzXA4HKis9PzwLi0tlVtozGYzzpw5c8FrlZWVeZQ5v+WmsrISTqfzgpaeZo899hisVqu8FRQUtO4Nt0dINPDz/wI9xwF2K/DebCDv2za9xMieLQt8EhERUdu1KegIIbBo0SJ8/PHH+Oqrr5CUlHTF55SXl6OgoADx8fEAgJSUFOh0OmzevFkuU1xcjAMHDmD8+PEAgHHjxsFqtWLXrl1ymZ07d8JqtXqUOXDgAIqLi+UymzZtgsFgQEpKykXrYjAYEBER4bH5VJARuGct0Hsi4KgBVv8UqCm94tOapTTNkLy/0Ip6BycOJCIiaqs2BZ2FCxfi/fffxwcffIDw8HCUlJSgpKQE9fVSZ9mamhosX74cO3bswMmTJ7F161bMnDkTsbGxuP322wEARqMR8+fPx7Jly7Blyxbs27cP99xzD4YOHYpJkyYBAAYNGoSpU6diwYIFyMrKQlZWFhYsWIAZM2ZgwIABAIDU1FQMHjwYaWlp2LdvH7Zs2YLly5djwYIFvg8wbaEPBe5aAxh7Ao31wJmDrX5qz+gQWIxBcLoEsnn7ioiIqM3aFHRee+01WK1WTJw4EfHx8fK2Zs0aAIBGo8H+/ftx2223oX///pg3bx769++PHTt2IDw8XH6dlStXYvbs2ZgzZw4mTJiAkJAQrF+/HhqNRi6zevVqDB06FKmpqUhNTcWwYcPw3nvvycc1Gg02bNiAoKAgTJgwAXPmzMHs2bPxwgsvdPSaeJ8uCAg3S187ai9f9hwqlQpj+8QAALb/eNYXNSMiIurSVEIIoXQllGKz2WA0GmG1Wn3fCvSv2cCJr4Hb/w4Mv7PVT/tP9mks/+h7XJMQiU8WTvBd/YiIiDqJtnx+c60rf9GHSo+OmjY9bVxTi87+QiuqG5zerhUREVGXxqDjL3LQaf2tKwDoHhmMxJgQuNwCu09W+KBiREREXReDjr+0M+gAwPjmfjrHy71ZIyIioi6PQcdf2nnrCgDG9YkFAOw4waBDRETUFgw6/qIPkx6ddW1+6tje0lpiucU2VNU5vFkrIiKiLo1Bx186cOvKFB6EfqYwCAFknWA/HSIiotZi0PGXDgQdoKWfzg7Op0NERNRqDDr+0nzrqh19dICWYebbf2Q/HSIiotZi0PGXDrbojEmKgUoFHCutQVm13YsVIyIi6roYdPylg0EnKlSPQWZp9keOviIiImodBh1/6eCtK+DcfjoMOkRERK3BoOMvHWzRAYDxfdkhmYiIqC0YdPzFC0FndK9oaNQqnCyvQ1FVvZcqRkRE1HUx6PhL862rxgbA1diulwgP0mFodyMAjr4iIiJqDQYdf2lu0QEAZ/tbdSY03b767jhvXxEREV0Jg46/aPSAWit93YHbV9f17QYA+PbYWQghvFEzIiKiLotBx19UKq/00xmZGIkQvQZna+w4XFLtpcoRERF1TQw6/uSFIeYGrQZjkqRFPr89VuaNWhEREXVZDDr+5IUWHQC4vl/L7SsiIiK6NAYdf/Ja0IkFAOzKq0CD09XRWhEREXVZDDr+5IVbVwDQ1xQGc0QQ7I1u7D5Z4YWKERERdU0MOv7kpRYdlUqF65padTJ5+4qIiOiSGHT8yUtBB2i5ffUNgw4REdElMej4kxx0OnbrCgAm9JWCzqFiG8qq7R1+PSIioq6IQcef5D46HW/RiQ0zYIglAgBnSSYiIroUBh1/8uKtKwByPx0OMyciIro4Bh1/8nLQuUGeT6eMy0EQERFdBIOOP3nx1hUApCRGwaBVo7TajqNnOt7vh4iIqKth0PEnXYj06KWgE6TTYExvaTVzLgdBRER0IQYdf/LyrSsAuL4v++kQERFdCoOOP3lpZuRzXd9fCjo788q5HAQREdF5GHT8yQctOgPiwhFvDEKD040dP5Z77XWJiIi6AgYdf/JB0FGpVJg0KA4AsPnQGa+9LhERUVfAoONPPgg6AHDLIBMAYMuhM3C7OcyciIioGYOOP53bR8eL896M6xODUL0GZ2x2HCiyeu11iYiIOjsGHX9qbtGBAJz1XntZg1aDG/pLkwd+mcvbV0RERM0YdPypeR4dwOu3r1r66ZR69XWJiIg6MwYdf1KrAZ33VjA/100DTVCrpNXMT1fWefW1iYiIOisGHX/zUYfk6FA9RiVGAwC2sFWHiIgIQBuDzooVKzB69GiEh4fDZDJh9uzZOHLkiEcZIQSefPJJWCwWBAcHY+LEiTh48KBHGbvdjsWLFyM2NhahoaGYNWsWTp8+7VGmsrISaWlpMBqNMBqNSEtLQ1VVlUeZ/Px8zJw5E6GhoYiNjcWSJUvgcDja8pb8z0dBBwAmDZZGX33JYeZEREQA2hh0tm3bhoULFyIrKwubN29GY2MjUlNTUVvb8qH93HPP4aWXXsKqVauwe/dumM1mTJ48GdXV1XKZpUuXYt26dUhPT0dmZiZqamowY8YMuFwtM/vOnTsXOTk5yMjIQEZGBnJycpCWliYfd7lcmD59Ompra5GZmYn09HSsXbsWy5Yt68j18D0fzI7crLmfTtaJclQ3OL3++kRERJ2O6IDS0lIBQGzbtk0IIYTb7RZms1k888wzcpmGhgZhNBrF66+/LoQQoqqqSuh0OpGeni6XKSwsFGq1WmRkZAghhMjNzRUARFZWllxmx44dAoA4fPiwEEKIjRs3CrVaLQoLC+UyH374oTAYDMJqtbaq/larVQBodXmveCtViCcihDj4X5+8/E0vfC0Sf/OZ+Oz7Ip+8PhERkdLa8vndoT46Vqs0Z0t0tNQ3JC8vDyUlJUhNTZXLGAwG3Hjjjdi+fTsAIDs7G06n06OMxWJBcnKyXGbHjh0wGo0YM2aMXGbs2LEwGo0eZZKTk2GxWOQyU6ZMgd1uR3Z29kXra7fbYbPZPDa/8+GtKwCY3NSqw9tXREREHeiMLITAI488guuuuw7JyckAgJKSEgBAXFycR9m4uDj5WElJCfR6PaKioi5bxmQyXXBOk8nkUeb880RFRUGv18tlzrdixQq5z4/RaERCQkJb33bH6X0z6qrZpMHSNfnqcCkaXW6fnIOIiKizaHfQWbRoEX744Qd8+OGHFxxTqVQe3wshLth3vvPLXKx8e8qc67HHHoPVapW3goKCy9bJJ+Q+Or5p0RnZMwpRITpY653Yc6rSJ+cgIiLqLNoVdBYvXoxPP/0UX3/9NXr06CHvN5vNAHBBi0ppaanc+mI2m+FwOFBZWXnZMmfOXHjrpayszKPM+eeprKyE0+m8oKWnmcFgQEREhMfmdz6+daVRq3DzQOn9f3Hw4i1bREREV4s2BR0hBBYtWoSPP/4YX331FZKSkjyOJyUlwWw2Y/PmzfI+h8OBbdu2Yfz48QCAlJQU6HQ6jzLFxcU4cOCAXGbcuHGwWq3YtWuXXGbnzp2wWq0eZQ4cOIDi4mK5zKZNm2AwGJCSktKWt+VfPg46ADA1WQqcG/cXw8VFPomI6CqmbUvhhQsX4oMPPsB///tfhIeHyy0qRqMRwcHBUKlUWLp0KZ5++mn069cP/fr1w9NPP42QkBDMnTtXLjt//nwsW7YMMTExiI6OxvLlyzF06FBMmjQJADBo0CBMnToVCxYswBtvvAEAuO+++zBjxgwMGDAAAJCamorBgwcjLS0Nzz//PCoqKrB8+XIsWLBAmZaa1vLh8PJmN/SPRUSQFmdsduw+WYGxvWN8di4iIqJA1qYWnddeew1WqxUTJ05EfHy8vK1Zs0Yu8+ijj2Lp0qV48MEHMWrUKBQWFmLTpk0IDw+Xy6xcuRKzZ8/GnDlzMGHCBISEhGD9+vXQaDRymdWrV2Po0KFITU1Famoqhg0bhvfee08+rtFosGHDBgQFBWHChAmYM2cOZs+ejRdeeKEj18P3/NCiY9Bq5FadT78v8tl5iIiIAp1KCHHV3tuw2WwwGo2wWq3+awXK/iewfgnQfxowN91np8k8dhb3vL0TUSE67PrdJOg0XO2DiIi6hrZ8fvPTz9+aW3ScvmvRAYCxvaMRG6ZHZZ0TmcfP+vRcREREgYpBx998PLy8mVajxvSh8QCA9bx9RUREVykGHX/zQx+dZrOukWaN3nTwDBqcriuUJiIi6noYdPzNj0FnREIUukcGo8beiK1HSn1+PiIiokDDoONvfhhe3kytVmHGcOn2FUdfERHR1YhBx9/0IdKjH1p0AGDmMOn21ZZDpaixN/rlnERERIGCQcffmm9duRxAo8PnpxtiiUDvbqGwN7qxOZdLQhAR0dWFQcffdKEtX/t4iDkgLXza3Kqz/vviK5QmIiLqWhh0/E2rBzR66Wt/3b4aLgWdb46WoaLW961IREREgYJBRwl+HHkFAH1NYRja3YhGt8B/cwr9ck4iIqJAwKCjBD+OvGr2Pyk9AAAf7Tntt3MSEREpjUFHCX5u0QGA266xQK9RI7fYhoNFVr+dl4iISEkMOkpQIOhEhugxeXAcALbqEBHR1YNBRwkKBB0A+Oko6fbVf3MK4Wh0+/XcRERESmDQUYICfXQA4Pp+3WCOCEJlnRNbDp3x67mJiIiUwKCjBIVadDRqFX4ysjsA4KNs3r4iIqKuj0FHCQoFHaBl9NXWI6U4Y2vw+/mJiIj8iUFHCQrdugKA3t3CMCoxCm4BfLyXc+oQEVHXxqCjBAVbdICWTskfZRdACKFIHYiIiPyBQUcJCged6cMsCNZpcKKsFnvzqxSpAxERkT8w6ChBDjr+v3UFAGEGLaYNNQMA/pNdoEgdiIiI/IFBRwlyHx1lWnQAYM6oBADApzlFqLU3KlYPIiIiX2LQUYLCt64AYExSNHrFhKDW4cKGH4oVqwcREZEvMegoQQ46dYpVQaVS4c7RPQEA6bvzFasHERGRLzHoKEHB4eXnuiOlOzRqFfbmV+HomWpF60JEROQLDDpKCIBbVwBgCg/CLQNNAIA1u9kpmYiIuh4GHSUESNABgJ9dK3VK/njvadgbXQrXhoiIyLsYdJTQfOvKWQu4lV1F/Mb+Jnmhz825XOiTiIi6FgYdJTS36ACAU7kOyYC00OecppmSefuKiIi6GgYdJWiDAFXTpQ+A21c/HZUAlQr49thZFFQoG7yIiIi8iUFHCSpVwIy8AoCE6BBc1zcWAPDRHrbqEBFR18GgoxRdiPR4botOox2oOKFIde4cLXVK/vee03C5udAnERF1DQw6SrnYyKtPHgD+OgI4vcfv1Zk8OA5RITqU2Brw5SF2SiYioq6BQUcp5wcd62ng4DrpawWCjkGrwc+ulWZK/kdmnt/PT0RE5AsMOko5v49OzgeAaBpqXl2kSJV+Pi4RWrUKO/MqcKDQqkgdiIiIvIlBRynntui43cDe91qO2ZRZZDPeGIxbh8YDAP7xHVt1iIio82PQUcq5QefE14D1nIU1q5VbTfwX1yUBANZ/X4TS6gbF6kFEROQNDDpKOffW1d5/SV/HJUuPNmVuXQHANQmRSEmMgtMl8P6OU4rVg4iIyBvaHHS++eYbzJw5ExaLBSqVCp988onH8XvvvRcqlcpjGzt2rEcZu92OxYsXIzY2FqGhoZg1axZOnz7tUaayshJpaWkwGo0wGo1IS0tDVVWVR5n8/HzMnDkToaGhiI2NxZIlS+BwONr6lpTR3KJTdQo4vEH6+sZHpcfqEkAoN8T7FxOkVp33d+ajwcn1r4iIqPNqc9Cpra3F8OHDsWrVqkuWmTp1KoqLi+Vt48aNHseXLl2KdevWIT09HZmZmaipqcGMGTPgcrV8qM6dOxc5OTnIyMhARkYGcnJykJaWJh93uVyYPn06amtrkZmZifT0dKxduxbLli1r61tSRnPQ+eEjwO0ELCOAvpOlfc5awG5TrGpThsShe2QwKmod+G9OoWL1ICIi6ihtW58wbdo0TJs27bJlDAYDzGbzRY9ZrVa8/fbbeO+99zBp0iQAwPvvv4+EhAR8+eWXmDJlCg4dOoSMjAxkZWVhzJgxAIA333wT48aNw5EjRzBgwABs2rQJubm5KCgogMViAQC8+OKLuPfee/HnP/8ZERERbX1r/tUcdJxNw8tH/hzQhwBBRqDBKnVIDjIqUjWtRo154xPx9MbDeDszD3NGJUClUilSFyIioo7wSR+drVu3wmQyoX///liwYAFKS0vlY9nZ2XA6nUhNTZX3WSwWJCcnY/v27QCAHTt2wGg0yiEHAMaOHQuj0ehRJjk5WQ45ADBlyhTY7XZkZ2dftF52ux02m81jU0xzHx1AmiU5+X+kr8Ob3o9CQ8yb3Tm6J0L0Ghw9U4PvjpcrWhciIqL28nrQmTZtGlavXo2vvvoKL774Inbv3o2bb74ZdrsdAFBSUgK9Xo+oqCiP58XFxaGkpEQuYzKZLnhtk8nkUSYuLs7jeFRUFPR6vVzmfCtWrJD7/BiNRiQkJHT4/bbbuSuYD/kJENTUAhUhDe9Waoh5M2OwDnNGSdfnzW+VWZaCiIioo7wedO68805Mnz4dycnJmDlzJj7//HMcPXoUGzZsuOzzhBAet0cudqukPWXO9dhjj8FqtcpbQYGCC1ieG3RG/rzl6wBp0QGA/zehF9QqYNvRMhws4gSCRETU+fh8eHl8fDwSExNx7NgxAIDZbIbD4UBlZaVHudLSUrmFxmw248yZC9dbKisr8yhzfstNZWUlnE7nBS09zQwGAyIiIjw2xURKyy3ANARIuLZlf3OLTvXFW6X8KTEmFDOGScHr1a9/VLg2REREbefzoFNeXo6CggLEx0sf4CkpKdDpdNi8ebNcpri4GAcOHMD48eMBAOPGjYPVasWuXbvkMjt37oTVavUoc+DAARQXt9zi2bRpEwwGA1JSUnz9tjquxyjgrnTg7n8D57ZAhTd14lb41lWzB2/qAwDYeKAYP5bVKFwbIiKitmlz0KmpqUFOTg5ycnIAAHl5ecjJyUF+fj5qamqwfPly7NixAydPnsTWrVsxc+ZMxMbG4vbbbwcAGI1GzJ8/H8uWLcOWLVuwb98+3HPPPRg6dKg8CmvQoEGYOnUqFixYgKysLGRlZWHBggWYMWMGBgwYAABITU3F4MGDkZaWhn379mHLli1Yvnw5FixYEPgjrpoNmAYYe3juC6BbVwAw0ByBSYPiIATw+la26hARUefS5qCzZ88ejBgxAiNGjAAAPPLIIxgxYgT+8Ic/QKPRYP/+/bjtttvQv39/zJs3D/3798eOHTsQHh4uv8bKlSsxe/ZszJkzBxMmTEBISAjWr18PjUYjl1m9ejWGDh2K1NRUpKamYtiwYXjvvZb1oDQaDTZs2ICgoCBMmDABc+bMwezZs/HCCy905HooL0A6I59rYVOrzrp9hThdWadwbYiIiFpPJYSCU/AqzGazwWg0wmq1Bk4rUPUZ4MX+gEoN/G8ZoGnzVEc+cfdbWfjueDnmjUvEH29LVro6RER0FWvL5zfXugo0od0AtRYQbqDmwg7ZSlk4sS8AIH13Acqq7QrXhoiIqHUYdAKNWg2ENXVIVnAV8/ON6xODaxIiYW904+3MPKWrQ0RE1CoMOoFIHmIeOEFHpVJh0U1Sq877WadgrXMqXCMiIqIrY9AJRAE2xLzZzQNNGGgOR429EW9lcrZkIiIKfAw6gSjAhpg3U6tVWDqpPwDg7cw8nK1hXx0iIgpsDDqBKACHmDebMiQOw3oYUedwcbZkIiIKeAw6gShAW3QAqa/Or6dIkza+n3UKhVX1CteIiIjo0hh0AlEAt+gAwHV9YzGudwwcLjf++uUxpatDRER0SQw6gUhu0QnMoKNSqbC8qVXnP3tPcw0sIiIKWAw6gah51JWjBrBXK1uXS0hJjMKkQSa43AIrNx9VujpEREQXxaATiAxhgKFpSusAvX0FAMtSB0ClAj77oRgHCq1KV4eIiOgCDDqBKrx50sDA65DcbFB8BGYNl26zPf/FEYVrQ0REdCEGnUAV4B2Smz08qT+0ahW2HS3D1iOlSleHiIjIA4NOoArgIebn6hUbiv83oRcA4KnPcuF0uZWtEBER0TkYdAJVJ2nRAYDFt/RDTKgeJ8pq8a8dp5SuDhERkYxBJ1CFB97CnpcSEaSTJxF8+cujKOfSEEREFCAYdAJVJwo6APDTUQlI7h6B6oZGvLCJw82JiCgwMOgEqk506woANGoVnpg5BACQvjsfB4s43JyIiJTHoBOomjsj15wB3K6W/d+vAf4yHCjap0y9LmN0r2jMHG6BEMAfP82FEELpKhER0VWOQSdQhZkAlQYQLqCmadh27Vlg43Kg8iSw91+KVu9SHps2EEE6NXadrMCn3wf2iDEiIur6GHQClVoDhMVJXzcPMd/6DGC3SV+fzFSmXldgiQzGgxP7AgD+9FkuquocCteIiIiuZgw6gax5zStbMVB2BNjzj5ZjZ48C1SXK1OsKfnVjb/Q1heFsjQN/3nBI6eoQEdFVjEEnkEWcs4r5pt9Lt7EG3AqYh0n7A7RVx6DV4Nk7hkKlAj7KPo3tx88qXSUiIrpKMegEsuYh5j/8Gzj2BaDWApOfAnpdL+0P0KADACmJ0bhnTCIA4LF1+9HgdF3hGURERN7HoBPImoeYn94lPY7+JRDbD+h1nfR9AAcdAHh06gCYI4JwqrwOf9lyTOnqEBHRVYhBJ5A1DzEHgCAjcONvpK8TxwFQAeXHArafDgCEB+nw1G3S3Dp//+YEcotsCteIiIiuNgw6gay5RQeQQk5ItPR1cBQQH9j9dJqlDjHj1qFmuNwCv/34B7jcnFuHiIj8h0EnkJmGALpQwDQYGL3A81gn6KfT7MmZQxAepMUPp61457s8patDRERXEQadQBbWDVj6AzB/M6DVex7rJP10AMAUEYTf3ToIAPDipqMoqKhTuEZERHS1YNAJdKGxgCHswv09O0c/nWZ3jk7A2N7RqHe68Pi6/VwegoiI/IJBp7MKjuw0/XQAQKVSYcVPhsGgVePbY2exdm+h0lUiIqKrAINOZyb30/lW2Xq0UlJsKJZO6g9AWh6irNqucI2IiKirY9DpzDpRP51mC65PwhBLBKz1Tjz1Wa7S1SEioi6OQaczk/vpHJfWw+oEtBo1nr1jGDRqFdZ/X4TNuWeUrhIREXVhDDqd2bn9dE5917K/0QGUHgLcbkWqdSXJ3Y345fVJAIBH//M9iq31CteIiIi6Kgadzq65n87xL4FD64G1C4Dn+wCvjgW2rlC2bpfxyOT+SO4egco6Jx76MAeNrsAMZURE1Lkx6HR2zf10vv8QWHMPsP/fgL1pqYUdq4CaUuXqdhkGrQar7hqJMIMWu05WcC0sIiLyCQadzi5xPGAwSl8bE4CxC4FffAF0HwU464Dv/qJs/S6jV2wonv7JUADAqq+PI/PYWYVrREREXU2bg84333yDmTNnwmKxQKVS4ZNPPvE4LoTAk08+CYvFguDgYEycOBEHDx70KGO327F48WLExsYiNDQUs2bNwunTpz3KVFZWIi0tDUajEUajEWlpaaiqqvIok5+fj5kzZyI0NBSxsbFYsmQJHA5HW99S5xZkBH61DfjVt8DS/cDUp4GeY4GbHpOO734roCcUnDXcgruuTYAQwNI1OSitblC6SkRE1IW0OejU1tZi+PDhWLVq1UWPP/fcc3jppZewatUq7N69G2azGZMnT0Z1dbVcZunSpVi3bh3S09ORmZmJmpoazJgxAy6XSy4zd+5c5OTkICMjAxkZGcjJyUFaWpp83OVyYfr06aitrUVmZibS09Oxdu1aLFu2rK1vqfOLTpI6JatULfv63AL0uBZobAAyX1asaq3xhxlDMCAuHGdr7Hh4TQ4X/iQiIu8RHQBArFu3Tv7e7XYLs9ksnnnmGXlfQ0ODMBqN4vXXXxdCCFFVVSV0Op1IT0+XyxQWFgq1Wi0yMjKEEELk5uYKACIrK0sus2PHDgFAHD58WAghxMaNG4VarRaFhYVymQ8//FAYDAZhtVpbVX+r1SoAtLp8p3P8KyGeiBDiqW5CWIuUrs1lHS2xiYH/+7lI/M1n4pUtR5WuDhERBbC2fH57tY9OXl4eSkpKkJqaKu8zGAy48cYbsX37dgBAdnY2nE6nRxmLxYLk5GS5zI4dO2A0GjFmzBi5zNixY2E0Gj3KJCcnw2KxyGWmTJkCu92O7Ozsi9bPbrfDZrN5bF1a74nSXDsuO5D5ktK1uax+ceH4421DAAAvbT6KXXkVCteIiIi6Aq8GnZISqS9IXFycx/64uDj5WElJCfR6PaKioi5bxmQyXfD6JpPJo8z554mKioJer5fLnG/FihVynx+j0YiEhIR2vMtORKUCJjb11cl+F7Ces76UoxYoypHm3AkQP03pgdtHdIdbAEs+3IeK2sCpGxERdU4+GXWlOrevCKQOyufvO9/5ZS5Wvj1lzvXYY4/BarXKW0FBwWXr1CUk3QAkXge4HMBnDwMblgFv3ACsSAD+fiPw/k+AxsBYc0qlUuFPs5PROzYUJbYG/Pqj77nKORERdYhXg47ZbAaAC1pUSktL5dYXs9kMh8OBysrKy5Y5c+bCpQHKyso8ypx/nsrKSjidzgtaepoZDAZERER4bF2eStUyAuvYF9IorOLvAeECoJIWBP3kwYCZRTnMoMUrc0dAr1Vjy+FSvJ2Zp3SViIioE/Nq0ElKSoLZbMbmzZvlfQ6HA9u2bcP48eMBACkpKdDpdB5liouLceDAAbnMuHHjYLVasWvXLrnMzp07YbVaPcocOHAAxcUtazxt2rQJBoMBKSkp3nxbnV+v64Bxi6T+OmMXAj99F3j4IJD2MaDWAgf+A3z5hNK1lA2xGPH76YMAAM9mHEZOQZWyFSIiok5LJdp4b6CmpgbHjx8HAIwYMQIvvfQSbrrpJkRHR6Nnz5549tlnsWLFCrzzzjvo168fnn76aWzduhVHjhxBeHg4AOCBBx7AZ599hnfffRfR0dFYvnw5ysvLkZ2dDY1GAwCYNm0aioqK8MYbbwAA7rvvPiQmJmL9+vUApOHl11xzDeLi4vD888+joqIC9957L2bPno1XXnmlVe/FZrPBaDTCarVeHa07F/N9OrDuV9LXU58Fxt6vbH2aCCHw4Oq9+PxACeIiDFj34ARYIoOVrhYREQWANn1+t3VI19dffy0AXLDNmzdPCCENMX/iiSeE2WwWBoNB3HDDDWL//v0er1FfXy8WLVokoqOjRXBwsJgxY4bIz8/3KFNeXi7uvvtuER4eLsLDw8Xdd98tKisrPcqcOnVKTJ8+XQQHB4vo6GixaNEi0dDQ0Or30uWHl7fWNy9Iw9CfMApx8BOlayOz1jvEpBe3isTffCamrNwmbPUOpatEREQBoC2f321u0elK2KLTRAipk/KetwGVBuhzEzB0DjBwOmAIU7RqpyvrcPur21FWbccN/bvh7XmjoNNw5RIioqtZWz6/+YlBUoflW58Hht0pdVI+/iWw7j7g+b7Af34BnMyUwpACekSF4O15oxCs0+Cbo2X4w38PcCQWERG1GoMOSdQa4Cd/Bxbvlebeie4NNNYDB9YC704H3pkmBSAFQsawHpH4610joFIBH+4qwOvbTvi9DkRE1Dnx1hVvXV2cEEDhXmDfe0DOamkeHgCwjARuWA70nwao/ZuT3/kuD39cnwsAeGnOcPxkZA+/np+IiAJDWz6/GXQYdK7MVgxsfwXY8w+plQeQWnyuvQ+4Zq60grqf/OmzXLydmQeNWoU3f56CmwdefM4kIiLquhh0WolBp41qyoCsvwG7/wHYrdI+fRgw/C5g/CIgqpfPq+B2Cyz76Hus21cIg1aN9385BqN7Rfv8vEREFDgYdFqJQaed7DXAD2uAnW8AZ49I+wwRwE/fAfpO8vnpnS43fvVeNr46XIrwIC3+/atxGBTPnx8R0dWCo67ItwxhwOj5wMKdQNonQI9rAbsNWD0H2PWmz0+v06jxt7kjMbpXFKobGvHzf+zCqfJan5+XiIg6HwYdaj+VSppz597PgOFzpaHpG5cDGx8FXI0+PXWwXoO35o3GQHM4yqrtuOftnSiqqvfpOYmIqPPhrSveuvIOIYDMlcCWP0rfJ90IJFwrjdZyOaVHXQhg7AFEWICI7kBkIhAa06HTltoaMOeNHThZXoek2FCsuW8sTBFBXnhDREQUqNhHp5UYdHwg91Pg4/taRmddyYh7gGnPAfrQdp+ysKoec17fgcKqevQzhSH9vrGICTO0+/WIiCiwMei0EoOOj5TsB/atBoQb0OikTa0DHDWA9TRgKwJshUB108rzMf2A//kHED+s3afML6/DnDd2oMTWgEHxEfhwwRhEhui99IaIiCiQMOi0EoOOwvK+BT5eIAUejR6Y/CdgzK+kvj/tcKKsBnPeyMLZGjuG9TDivfljYAzWebnSRESkNAadVmLQCQC15cB/FwJHP5e+Nw+T+vHow6TRXUFGqb9P0o2tmon56Jlq/OzvWaiodWCIJQLvzR+D6FC27BARdSUMOq3EoBMghAB2/R3Y9L8tS02cz5gAXHO3NBNzVOJlX+5QsQ1pb+/E2RoH+pnCsPqXY9hBmYioC2HQaSUGnQBTeRI4vQewV0v9eew1Ul+e3E9bZmIGgLihgEYLuF1SSFIBGPITYMJSudXneGkN7nlrJ0psDUiMCcHqX45Bj6gQJd4VERF5GYNOKzHodBLOeuDwBmmB0RNbL12uXyrwkzeB4EgAQEFFHea+lYWCinpYjEFYvWAskmLbP7qLiIgCA4NOKzHodEJV+cCZXECllja1Gjh7HNj8e6CxAYhKAn62GogbAgAosTbg7rey8GNZLWLDDPjnL0ZjiMV/i5ASEZH3Mei0EoNOF1KUA6xJA6z50sSEs14Bhv4PAOBsjR0/f3sXcottCDdo8ea8URjbu2MTFRIRkXK41hVdfSzXAL/aBvS+CXDWAWvnS0tRNNoRG2ZA+q/G4tqkaFTbpbWxNh0sUbrGRETkBww61HWERAP3rAWue1j6ftcbwD+mApUnERGkw79+cS0mDYqDo9GN+9/fg493HFK2vkRE5HO8dcVbV13TkQzgk/uB+kppLp7bXgV63wjX8a3YszkdiZXfwayqxJnQAeh27Ryoh8wGYvsqXWsiImoF9tFpJQadLq6qAPjP/wNO75a+V+sAt/PS5eOSgeQ7gBFpQFg3/9SxK3O7WhZ1FS7AYGzVpI9ERFfCoNNKDDpXgUaHtKL6jlXS91FJQP8pQL/JWF8cie1frMFUVRYmaA5CC5dURq0DhswGRv8SSBjT7iUpOp3qM4AhHNC3cb6hBhtQtFeaA6kwW3qsLQNw3p8WlRoIjgZCY4HQbtLq9YnjgMTx0s/larnORNRhDDqtxKBzFanIk1oYYvp4fKDuOVmB+9/PhrOmAncE78MjMTsQdjan5XndBgIR3QG1BlBppEd9KBBmAsLMQFgcEBojfdjXnJG26jPSLTNnnTTk3VkntWr0HAuMWyTVIVA02oFD64HdbwP526WQ1z0F6HUd0GuCtCSHowaorwIarNL7shYA5T8CFT9K19V6GheEmrYKj5dCpT6sqRWoaRMC0AVJI+m0QYAuGHA3SvV2OaRHtUaaOTuql7RF9gQctUBlHlBxQqpjfSVgGgx0HwlYRkj9uYio02LQaSUGHQKAwqp6LPjnHuQW26BRq/DMOBf+x5UB1YH/SEHFq1TAoBnA+IeAhNEde6lGO5C/Q1otPi4Z6DlOCgXnq6+UyjTapeAAIT0W7JQmYawt61g9AClcdB8F9BglPUYlSgu1anTSI1RSPWrLgLqzQE0ZUHoQOLUdKNx7+VuKvhCVBEQmtIRXlUZqcXI3SnVxu6RwqtZIrVzNm75p/bUgIxAUKT0awqUApgtpedSHAlrDha1ULqd0HZr7joXFsSWLqB0YdFqJQYea1Tka8Zu1+7H++yIAwIS+MXh5ZiK6le2Qwo7bJfUzEW5piYqa0qbWmxKgrhwwRADhcdIHV1gcEBLT9MEXBGiDpdaHvf8Cjn3RclLTYOnDsNHe0kIRYQF6jJYCQ4/R0gKnQkjnrK+QznV6D3B8C3DyW6m1qJk2WGqF6XMLEBQBFOyStrIrjC4LjwdGzgNS5knv9eR3wMlM4NR3UuuNNliabbr5wz3CIrVKRfcGovsAMX2lVq32ctQBhXukwAPRFJCaQhJULa1izgagsR5Qa1vKaA3SdavKl5YQqTwlfa0PkcJMdJJUT0OEFPaK9kqtPP6g1jYtThsuBab6SqlV7Fy60KbrmCSFRbW2aTJMlfTocja9//qm38NG6fpH9255fyExTQHN3fQ7KqRbg129P5TbLV1TXZAULH1BCKCuoqll8ARgtwHBUU3/Hpoehbvp52OXfj/djdLPUa2TfofVGmk5m7pyKeTXVUivozE0tVIGSY8qVdPPsGlr/regbTquNUjvMyiy6fyR0r9zje7CejsbgOpiaastk/4mdRsozxrv8f5qzkjvTaWRfpdCoqXXVqulvzvVZ4CaEulvndYg/f5FdG/6HdP45rq3AoNOKzHo0LmEEPhoz2k88elB1DtdiAnV44U5w3HTAJP3TlJ6CNi+CvhhTetaMYKMUhC4VNmwOMAyEijaJ/0xupTIxKY/cqqWFoRQEzDibmDArRf/YwlIH7SXOhaohLh8K0l9pXS96ipaAqy7KcSqtS0fTs2d1+01LeuvNdikddcaztns1dIHi7NO+sBz2a9cxyCj9Dzh9t77Ppc+TGrlMycD5qFNt2At0u1Wrb6lnNstffjaCoHasy23A13OlvfR3Nql1kjXJqSpj1WYSfpAdNml25dVTSHTVtzUKnZO+GpskH6PnXXSbcVGu9T6FRTR1FoWIf3cqouk51cXSf+Z0Bik39vmcKHRt/wno6ZUem2g6T8aZmkLNUlBVxvc1MIWfF6LW7D0Og02oKGq6ZZsVdPPsbapnvXSz7ky33OdvUCk0jQFIb10vVx26Xf8YsLjpd+FkGjp9nP5j4Cj+iKvqZZeq7H+0udVa5v+U9cUjIKjpC3MJN1KjkwAjD2l/6xdrKW5gxh0WolBhy7meGkNFn+4D4eKbQCA+dcl4dGpA2DQevF/L7ZiqeOuRif9L0ljkL4uPy61wpzeDZw52PKHHJD+mAVHS8Pg+9wC9L1F+jBTqaQPidJDwI9bgB+/lj5YeoyS+r30uJajyPzJ7WpZlNbRFJJcTs//LWu0Ukf5qlPS/6bLf5Q+3N1N/5MXbul1NLqWvkm6YAAqqZWtIk/qg1R5UgonbRHaTQo8dmtLKGkvlcbzd7SriugutaIFGVuCUUOV9KjWeLbMqLVSyHM5pWvragQMYU0//6bNEN7UWtfUEuSsByCaWvM0LS0lzcGzuUXPUdvSX+5iAeVc2iAp2IR2k4KsrfDi5VRqqTURaGltOpc+vKm12iyFKFuR1FLUlpA+Ig24bVXry7cCg04rMejQpTQ4XXjm88N4d/tJAEBy9wj89Wcj0LtbmP8q4aiVbsUERUgBp62joajrc7ulD0C1tqmvUVM/o/Lj0q265q3iR+nWw0VDkaqpJaSb9OEo963SNb1eU2uXcEkfunXlUmtKQ1XLS+jDpFbDyASp5Ugb5NmBX2OQfn/l/ktBUutOg036YLXbpLAeYZE+nCMsUmuBy9kUKCqlD3iXven2sKlpIEA3KQRUl7TcXqktk/bJ2zmDApr3NdqlsNF8Cyg4sqmvVagUKPUhTe+pp9TBXRfs+59lW7kapevW2HBO5/wGqSUy3Cy1rpzbstlgBcqOAGWHpWsZ3Vu67RydJP1nq1mjQ7pN7qyTWscMF/mb52psuXXf3OeseasulsK49bQ0xYezFhi/GEj9P6++fQadVmLQoSv5MvcMfv2f71FZ50SIXoM/3ZaMO1J6KF0torYTQgoptqbbQkERLYGiPbcnGx3SbS9t0IUfqkSA9DtXX9nUb8y76wsy6LQSgw61Rom1AUvX7EPWiQoAwOxrLPjjrGQYQzpZ3xUioi6Ci3oSeZHZGITVvxyL5an9oVGr8ElOESav3IYth84oXTUiIroCBh2iVtCoVVh0cz98dP849O4WitJqO+b/cw8eWZMDa52f54AhIqJWY9AhaoORPaOwccn1+NUNvaFWAR/vK8TkldvwxcHLDO0mIiLFMOgQtVGQToPHbh2E/zwwHn2aWnd+9V427n8vG2ds3p5JmYiIOoJBh6idRvaMwoYl1+PBiX2gVauQcbAEk17ahg925sPtvmr7+BMRBRQGHaIOCNJp8OjUgVi/+DoM72FEdUMjHl+3H3f+fQcOFAb4jKpERFcBDi/n8HLyEpdb4N3tJ/HipiOoc7igUgG3j+iOX08ZgHhjAE44RkTUSSk6vPzJJ5+ESqXy2Mxms3xcCIEnn3wSFosFwcHBmDhxIg4ePOjxGna7HYsXL0ZsbCxCQ0Mxa9YsnD592qNMZWUl0tLSYDQaYTQakZaWhqqqKm+/HaJW06hVmH9dEjY/ciNmX2OBEMDHewtx0wtb8eKmI6ixNypdRSKiq45Pbl0NGTIExcXF8rZ//3752HPPPYeXXnoJq1atwu7du2E2mzF58mRUV7es27F06VKsW7cO6enpyMzMRE1NDWbMmAGXq2VNlblz5yInJwcZGRnIyMhATk4O0tLSfPF2iNqke2QwXv7ZCPx34QRc2ysaDU43XvnqOG56YSvW7M6Hi/13iIj8xuu3rp588kl88sknyMnJueCYEAIWiwVLly7Fb37zGwBS601cXByeffZZ/OpXv4LVakW3bt3w3nvv4c477wQAFBUVISEhARs3bsSUKVNw6NAhDB48GFlZWRgzZgwAICsrC+PGjcPhw4cxYMCAVtWVt67I14QQ+OLgGTzz+SGcLK8DAAyOj8D/zhiE8X1iFa4dEVHnpPjMyMeOHYPFYkFSUhJ+9rOf4cSJEwCAvLw8lJSUIDU1VS5rMBhw4403Yvv27QCA7OxsOJ1OjzIWiwXJyclymR07dsBoNMohBwDGjh0Lo9Eol7kYu90Om83msRH5kkqlwtRkMzY9fCP+d/oghAdpkVtsw9w3d+KX/9yD46VXWIGYiIg6xOtBZ8yYMfjXv/6FL774Am+++SZKSkowfvx4lJeXo6REmlQtLi7O4zlxcXHysZKSEuj1ekRFRV22jMlkuuDcJpNJLnMxK1askPv0GI1GJCQkdOi9ErWWXqvGL6/vjW2/vgk/H5cIjVqFLw+dQerKb/Drj75HYVW90lUkIuqSvB50pk2bhjvuuANDhw7FpEmTsGHDBgDAP//5T7mM6rxVboUQF+w73/llLlb+Sq/z2GOPwWq1yltBQUGr3hORt0SH6vHUbcn4Yun1mDIkDm4BfJR9Gjc9vxVPrc9FeY1d6SoSEXUpPp9HJzQ0FEOHDsWxY8fk0Vfnt7qUlpbKrTxmsxkOhwOVlZWXLXPmzIULKpaVlV3QWnQug8GAiIgIj41ICX1N4XgjbRTWPTge43rHwOFy4x/f5eG6Z7/GnzfkorSaMywTEXmDz4OO3W7HoUOHEB8fj6SkJJjNZmzevFk+7nA4sG3bNowfPx4AkJKSAp1O51GmuLgYBw4ckMuMGzcOVqsVu3btksvs3LkTVqtVLkPUGYzoGYUPFozBe/OvxbAeRtQ7XXjz2zxc/+zXePLTgyixMvAQEXWE10ddLV++HDNnzkTPnj1RWlqK//u//8O2bduwf/9+JCYm4tlnn8WKFSvwzjvvoF+/fnj66aexdetWHDlyBOHh4QCABx54AJ999hneffddREdHY/ny5SgvL0d2djY0Gg0A6RZZUVER3njjDQDAfffdh8TERKxfv77VdeWoKwokQghsPVqGv245hn35VQAAvUaNO1K6474b+iApNlTZChIRBYi2fH5rvX3y06dP46677sLZs2fRrVs3jB07FllZWUhMTAQAPProo6ivr8eDDz6IyspKjBkzBps2bZJDDgCsXLkSWq0Wc+bMQX19PW655Ra8++67csgBgNWrV2PJkiXy6KxZs2Zh1apV3n47RH6jUqlw0wATJvbvhu+Ol+OvW45h18kKfLirAOm7CzAt2Yz7b+yDYT0ila4qEVGnwSUg2KJDAWz3yQq8vvVHbDlcKu8b2zsad49JxJQhZui1XK6OiK4+bfn8ZtBh0KFO4EhJNd7Y9iP++32RPLNybJge/5OSgLnX9kTPmBCFa0hE5D8MOq3EoEOdTVFVPdJ3F2DN7nycsbUMRb++XyzuurYnJg2KYysPEXV5DDqtxKBDnVWjy40th0vxwc58fHOsDM3/imPD9LgjpQd+NronOy8TUZfFoNNKDDrUFRRU1GHN7gKs2VOAsuqWVp5rk6IxZ1QCbh1qRoje6+MOiIgUw6DTSgw61JU4XW58dbgUH+7KxzdHy9C8SHqoXoMZwyyYM7oHRvaMuuIs5EREgY5Bp5UYdKirKrbW4+O9hfj3ngKcalo1HQD6dAvFnFEJ+MnIHugWblCwhkRE7ceg00oMOtTVCSGw+2Ql1uwuwMb9xah3ugAAWrUKNw004bZrLLhlYByC9ZorvBIRUeBg0GklBh26mlQ3OPHZD8VYs7sAOQVV8v4QvQaTB8dh1nALru/XjaO2iCjgMei0EoMOXa2OlFTjvzmF+PT7IpyurJf3G4N1uHWoGTOHWTCmdww0avbnIaLAw6DTSgw6dLUTQiCnoAqffl+Ez34o9hi1ZQo3YPqweKQONmN0ryhoNWzpIaLAwKDTSgw6RC1cboGdJ8rx6fdF+PxACaz1TvmYMViHmweaMHlwHG7o3w1hBg5XJyLlMOi0EoMO0cU5Gt345mgZPj9Qgq8On0FlXUvo0WlUGN0rGjcPNGHiABP6dAvlkHUi8isGnVZi0CG6skaXG3vzq7A5twRfHipF3tlaj+M9ooJxQ/9uuKFfLMb1iYUxWKdQTYnoasGg00oMOkRtl3e2FluPlOLrI2XIOlEOR6NbPqZWAdckROKG/t0wcYAJw7oboWaHZiLyMgadVmLQIeqYOkcjsk6U45ujZ/HtsTL8WObZ2hMdqscN/WIxcYAJ4/vEwBQRpFBNiagrYdBpJQYdIu8qrKrHt0fLsPVIGTKPn0WNvdHjeFJsKK7tFY0xvaMxulc0ekQFs38PEbUZg04rMegQ+Y7T5Ub2qUpsPVKGb4+VIbfYhvP/2nQLN2Bkz0iM6BmFkT2jMLS7kbM0E9EVMei0EoMOkf9Y653Yc7ICO/MqsPNEOQ4W2dDo9vzzo1GrMNAcjhE9I3FNQhRG9IxE71iO6iIiTww6rcSgQ6SceocLB4qs2HuqEvvyq7A3vxKl50xY2CwqRNfU4hOJkYlRGNYjkvP4EF3lGHRaiUGHKHAIIVBsbUBOQRX25Vcip6AKP5y2wn7OqC4AUKmA3rGhGNYjEsN6GDG0uxH9zeGICOKwdqKrBYNOKzHoEAU2R6Mbh4ptyD5Viez8Suw7VYkia8NFy1qMQRhgDkd/czgGmSMwxBKBpNhQLl1B1AUx6LQSgw5R51NWbceBQit+OG3F/sIqHCyyofgS4cegVWOgORyD4iPQp1sYencLRVJsKBKiQ6BjACLqtBh0WolBh6hrsNY7cfRMNY6USNuhYhsOFdtQ63BdtLxWrULPmBD06RaGvqYw9O0Whj4mKQjxFhhR4GPQaSUGHaKuy+0WOFVRh9wiGw6X2HDibC3yymqRd7YW9c6LByAAiA0zoHdsKHp3C0Wv2FD0ipFagXpGh3DoO1GAYNBpJQYdoquP2y1QYmvAibJaHC+txvGyGvxYWovjZTUou8ior3PFG4PQMzoEiTEhSIyRwk/P6BAkRIcgKkTHYfBEfsKg00oMOkR0ruoGJ06ercOJszX4sawWp8prcfKs1Apka2i87HND9Rr0iApBQnQwukcGwxIZjO5R0tc9okIQG6ZnECLyEgadVmLQIaLWEEKgqs6JvPJa5JfX4VR5HU5VSF/nV9RddP6f84XoNXLrT2J0CCyRwTAbgxAXYUBcRBBM4UHQa9lBmqg12vL5zVm3iIiuQKVSISpUj6hQPUb2jLrgeIPThaKqehRU1qOgog5FVfUorKpHYaX0WGJrQJ3DhcMl1ThcUn3J80SF6GAKD4IpwoBu4VIAigs3NAUiaYsNMzAQEbUBgw4RUQcF6TTo3S0MvbuFXfS4o9GN05VS609+RR3yy+tQbG3AGVsDSmwNKLXZ4XC5UVnnRGWdE0fOXDoMAUBkiA7dwqQwZGoORPJmkMNSkI6dp4kYdIiIfEyvVV82CAkhUFnnRGl1A8qq7Si12VFabccZWwNKqxtQYm3AGZsdpdUNcLqk22hVdU4cK6257HnDg7QwhUuBKDpUj6gQPaJDW7ZuYQbEhBkQG6ZHZIgeGjX7EFHXw6BDRKQwlUolh4+B5kuXc7sFrPVOnK2xS4Go2t4UhOw4U92AM9YGnKmWWojsjW5UNzSiuqERP5bVtqIOQESQDsZgHSJDmh/1iAppeYwK0SPynMfIED0igrTsZE0BjUGHiKiTUKtb+gr1iwu/ZDkhBGwNjSirbkBptRSKquqcKK91oLLWgYpaB8pr7SivceBsjR2VdU4IIU28aK13Ir+i9XXSqFVSKArWwdgUkIzBOkQE6RARrG16bDoe3PR1iA7hQTqEG7RQsxWJfIxBh4ioi1GpVHLg6Gu6dCBq1tjUP0gKOg751lhlneOCx8o6J6xNj/VOF1xugYqm8NQeYQYtIoK0UvAJ0jZtOvkxzKBBmEGLUIPnsTBD83EtgnRqtirRJTHoEBFd5bQaNbo19eVpiwanC7Z6J6rqnU3hSApEtgYnbA2NsNU7pa1BOt7cYlRV74SjaVX6GnsjauyNwCXWK2sNjVqFEH1LIAo1aBGi0yDUoEGIXosQvQbBeo30qNMgWK9FmEEjlw0zSGWayzZ/zT5LXQODDhERtUuQToMgnQamiKA2P9fe6JL7EFU3OGGrlx6rGxpha3qssTei1t6IansjaprK1dgb5efV2KVJHF1uIe/zJoNWLYeeUIMUkIJ1agTrpH3S+5e+b/46SKeBQaeBQauGQdv0fdOjXEargUGnhkGrgb6pnFatYquUjzDoEBGR3xm0GhjCNIgNa1sr0rncboFaRyNq7S45FNXaG1HncKHW0Yh6h7S/3uFCvdOFOocLDU4Xah0u1DW1JMnPc7hQ73ChztEId9M0uvZGN+yN0m09X9OoVXJgCtZ7hiGDVi0Houbvg3RSSNJr1dBrzns8/+uLlNFpVNBp1PKm16ih06qgVUvHulLoYtAhIqJOSa1WNfXZ8d6K80II2BvdqGsKPdKjFIxqm4LS+cGpodEFu9MtHXNKX9sbXbA3SvsanG6PMg1OFxwuN5yuloUJXG7RchsvAOg0qqbw0xKELhamtE2BSaNWQaeRgpJWo4JO3XJsVK8ozBhmUey9MOgQERE1UalU8m2m6FC9T8/ldgs4XFL4sTe65QBV73ShweGC3eVuCU1Od9P3UllHoxSenI0CDlfzo7Tf3uhu+toFh/y1FKyajze63XA2uuF0C7m/1LmcLgGnywU4XB1+nw6Xm0GnI1599VU8//zzKC4uxpAhQ/Dyyy/j+uuvV7paREREl6VWqxCk1ig+g7UQAi63kIKQyw1nUzBqdEkhytHYst/udDftc8PhEmh0SeWc7qZHlxuNbmm/0yXQ6HZjeI9IRd9fpw46a9aswdKlS/Hqq69iwoQJeOONNzBt2jTk5uaiZ8+eSlePiIgo4KlUKmg1Kmg1QDC63rIhnXr18jFjxmDkyJF47bXX5H2DBg3C7NmzsWLFiis+n6uXExERdT5t+fzutEvgOhwOZGdnIzU11WN/amoqtm/fftHn2O122Gw2j42IiIi6rk4bdM6ePQuXy4W4uDiP/XFxcSgpKbnoc1asWAGj0ShvCQkJ/qgqERERKaTTBp1m54/1F0Jccvz/Y489BqvVKm8FBQX+qCIREREppNN2Ro6NjYVGo7mg9aa0tPSCVp5mBoMBBkP7J6ciIiKizqXTtujo9XqkpKRg8+bNHvs3b96M8ePHK1QrIiIiCiSdtkUHAB555BGkpaVh1KhRGDduHP7+978jPz8f999/v9JVIyIiogDQqYPOnXfeifLycjz11FMoLi5GcnIyNm7ciMTERKWrRkRERAGgU8+j01GcR4eIiKjzuSrm0SEiIiK6EgYdIiIi6rIYdIiIiKjLYtAhIiKiLqtTj7rqqOZ+2FzzioiIqPNo/txuzXiqqzroVFdXAwDXvCIiIuqEqqurYTQaL1vmqh5e7na7UVRUhPDw8Euuj9VeNpsNCQkJKCgo4NB1H+O19h9ea//htfYfXmv/8da1FkKguroaFosFavXle+Fc1S06arUaPXr08Ok5IiIi+A/HT3it/YfX2n94rf2H19p/vHGtr9SS04ydkYmIiKjLYtAhIiKiLotBx0cMBgOeeOIJGAwGpavS5fFa+w+vtf/wWvsPr7X/KHGtr+rOyERERNS1sUWHiIiIuiwGHSIiIuqyGHSIiIioy2LQISIioi6LQccHXn31VSQlJSEoKAgpKSn49ttvla5Sp7dixQqMHj0a4eHhMJlMmD17No4cOeJRRgiBJ598EhaLBcHBwZg4cSIOHjyoUI27jhUrVkClUmHp0qXyPl5r7yksLMQ999yDmJgYhISE4JprrkF2drZ8nNfaOxobG/G///u/SEpKQnBwMHr37o2nnnoKbrdbLsNr3T7ffPMNZs6cCYvFApVKhU8++cTjeGuuq91ux+LFixEbG4vQ0FDMmjULp0+f9k4FBXlVenq60Ol04s033xS5ubnioYceEqGhoeLUqVNKV61TmzJlinjnnXfEgQMHRE5Ojpg+fbro2bOnqKmpkcs888wzIjw8XKxdu1bs379f3HnnnSI+Pl7YbDYFa9657dq1S/Tq1UsMGzZMPPTQQ/J+XmvvqKioEImJieLee+8VO3fuFHl5eeLLL78Ux48fl8vwWnvH//3f/4mYmBjx2Wefiby8PPHRRx+JsLAw8fLLL8tleK3bZ+PGjeJ3v/udWLt2rQAg1q1b53G8Ndf1/vvvF927dxebN28We/fuFTfddJMYPny4aGxs7HD9GHS87NprrxX333+/x76BAweK3/72twrVqGsqLS0VAMS2bduEEEK43W5hNpvFM888I5dpaGgQRqNRvP7660pVs1Orrq4W/fr1E5s3bxY33nijHHR4rb3nN7/5jbjuuusueZzX2numT58ufvGLX3js+8lPfiLuueceIQSvtbecH3Rac12rqqqETqcT6enpcpnCwkKhVqtFRkZGh+vEW1de5HA4kJ2djdTUVI/9qamp2L59u0K16pqsVisAIDo6GgCQl5eHkpISj2tvMBhw44038tq308KFCzF9+nRMmjTJYz+vtfd8+umnGDVqFH7605/CZDJhxIgRePPNN+XjvNbec91112HLli04evQoAOD7779HZmYmbr31VgC81r7SmuuanZ0Np9PpUcZisSA5Odkr1/6qXtTT286ePQuXy4W4uDiP/XFxcSgpKVGoVl2PEAKPPPIIrrvuOiQnJwOAfH0vdu1PnTrl9zp2dunp6di7dy927959wTFea+85ceIEXnvtNTzyyCN4/PHHsWvXLixZsgQGgwE///nPea296De/+Q2sVisGDhwIjUYDl8uFP//5z7jrrrsA8PfaV1pzXUtKSqDX6xEVFXVBGW98djLo+IBKpfL4XghxwT5qv0WLFuGHH35AZmbmBcd47TuuoKAADz30EDZt2oSgoKBLluO17ji3241Ro0bh6aefBgCMGDECBw8exGuvvYaf//zncjle645bs2YN3n//fXzwwQcYMmQIcnJysHTpUlgsFsybN08ux2vtG+25rt669rx15UWxsbHQaDQXJNDS0tIL0iy1z+LFi/Hpp5/i66+/Ro8ePeT9ZrMZAHjtvSA7OxulpaVISUmBVquFVqvFtm3b8Ne//hVarVa+nrzWHRcfH4/Bgwd77Bs0aBDy8/MB8Pfam37961/jt7/9LX72s59h6NChSEtLw8MPP4wVK1YA4LX2ldZcV7PZDIfDgcrKykuW6QgGHS/S6/VISUnB5s2bPfZv3rwZ48ePV6hWXYMQAosWLcLHH3+Mr776CklJSR7Hk5KSYDabPa69w+HAtm3beO3b6JZbbsH+/fuRk5Mjb6NGjcLdd9+NnJwc9O7dm9faSyZMmHDBNAlHjx5FYmIiAP5ee1NdXR3Uas+PPI1GIw8v57X2jdZc15SUFOh0Oo8yxcXFOHDggHeufYe7M5OH5uHlb7/9tsjNzRVLly4VoaGh4uTJk0pXrVN74IEHhNFoFFu3bhXFxcXyVldXJ5d55plnhNFoFB9//LHYv3+/uOuuuzg01EvOHXUlBK+1t+zatUtotVrx5z//WRw7dkysXr1ahISEiPfff18uw2vtHfPmzRPdu3eXh5d//PHHIjY2Vjz66KNyGV7r9qmurhb79u0T+/btEwDESy+9JPbt2ydPq9Ka63r//feLHj16iC+//FLs3btX3HzzzRxeHsj+9re/icTERKHX68XIkSPlIdDUfgAuur3zzjtyGbfbLZ544glhNpuFwWAQN9xwg9i/f79yle5Czg86vNbes379epGcnCwMBoMYOHCg+Pvf/+5xnNfaO2w2m3jooYdEz549RVBQkOjdu7f43e9+J+x2u1yG17p9vv7664v+fZ43b54QonXXtb6+XixatEhER0eL4OBgMWPGDJGfn++V+qmEEKLj7UJEREREgYd9dIiIiKjLYtAhIiKiLotBh4iIiLosBh0iIiLqshh0iIiIqMti0CEiIqIui0GHiIiIuiwGHSIiIuqyGHSIiIioy2LQISIioi6LQYeIiIi6LAYdIiIi6rL+P1NzOkcxRyGdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gb_hist = GBModel.train_score_\n",
    "ada_hist = [mean_squared_error(y_test, y_pred) \n",
    "            for y_pred in adaModel.staged_predict(X_test)]\n",
    "#print(gb_hist)\n",
    "#print(ada_hist)\n",
    "plt.plot(gb_hist, label=\"gb_hist\")\n",
    "plt.plot(ada_hist, label=\"ada_hist\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 47.151\n",
      "test loss = 3938.543\n",
      "\n",
      "GB mse: 4514.827 +/- 792.815\n"
     ]
    }
   ],
   "source": [
    "GBModel = GradientBoostingRegressor(n_estimators=100, max_depth=5)\n",
    "\n",
    "GBModel.fit(X_train, y_train)\n",
    "y_train_pred = GBModel.predict(X_train)\n",
    "y_test_pred = GBModel.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "gb_cv_scores = -1 * cross_val_score(GBModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"GB mse: {gb_cv_scores.mean():.3f} +/- {gb_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.000\n",
      "test loss = 20040.855\n",
      "\n",
      "GB mse: 18404.607 +/- 1963.957\n"
     ]
    }
   ],
   "source": [
    "GBModel = GradientBoostingRegressor(n_estimators=100, max_depth=15)\n",
    "\n",
    "GBModel.fit(X_train, y_train)\n",
    "y_train_pred = GBModel.predict(X_train)\n",
    "y_test_pred = GBModel.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "gb_cv_scores = -1 * cross_val_score(GBModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"GB mse: {gb_cv_scores.mean():.3f} +/- {gb_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth augmente la tendence a overfit, ce qui réduit grandement les perfs sur les tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 17128.939\n",
      "test loss = 20269.475\n",
      "\n",
      "GB mse: 20810.167 +/- 1293.157\n"
     ]
    }
   ],
   "source": [
    "GBModel = GradientBoostingRegressor(n_estimators=10, max_depth=3)\n",
    "\n",
    "GBModel.fit(X_train, y_train)\n",
    "y_train_pred = GBModel.predict(X_train)\n",
    "y_test_pred = GBModel.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "gb_cv_scores = -1 * cross_val_score(GBModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"GB mse: {gb_cv_scores.mean():.3f} +/- {gb_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 76.179\n",
      "test loss = 2442.422\n",
      "\n",
      "GB mse: 2518.169 +/- 303.590\n"
     ]
    }
   ],
   "source": [
    "GBModel = GradientBoostingRegressor(n_estimators=300, max_depth=3)\n",
    "\n",
    "GBModel.fit(X_train, y_train)\n",
    "y_train_pred = GBModel.predict(X_train)\n",
    "y_test_pred = GBModel.predict(X_test)\n",
    "\n",
    "print(f\"train loss = {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"test loss = {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print()\n",
    "\n",
    "gb_cv_scores = -1 * cross_val_score(GBModel, X, y, cv=5, n_jobs=5, scoring=\"neg_mean_squared_error\")\n",
    "print(f\"GB mse: {gb_cv_scores.mean():.3f} +/- {gb_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenter n_estimators a un impacte positif sur les resultats \n",
    "# on diminue aussi la variance mais surtout les erreur (au cout de temps a train et predict + memoire)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
