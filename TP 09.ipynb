{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c2e6e4",
   "metadata": {},
   "source": [
    "# TP 9 : Entraîner un MLP sans autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e98c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887738c",
   "metadata": {},
   "source": [
    "On veut construire un MLP avec deux couches linéaires (et une fonction d'activation après chaque couche)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28b9a5",
   "metadata": {},
   "source": [
    "## Fonction d'activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ed252",
   "metadata": {},
   "source": [
    "Écrivez deux fonctions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b040dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x:torch.Tensor)->torch.Tensor:\n",
    "    # Implementation of tanh\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def dsigma(x:torch.Tensor)->torch.Tensor:\n",
    "    # Implementation of the first derivative of tanh\n",
    "    return 1.0 - torch.pow(torch.tanh(x), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "58365bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "s = torch.empty(4, 3).normal_().requires_grad_(True)\n",
    "l = torch.sum(sigma(s)) # has a grad of ones\n",
    "l.backward()\n",
    "\n",
    "print((dsigma(s.detach()) == s.grad).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f877b",
   "metadata": {},
   "source": [
    "qui prennent en entrée un tenseur de nombres à virgule flottante et renvoient un tenseur de même taille, obtenu en appliquant terme à terme respectivement tanh, et la première dérivée de tanh.    \n",
    "\n",
    "**Indication** : Les fonctions ne doivent pas avoir de boucle python, et utiliser en particulier `torch.tanh`, `torch.exp`, `torch.mul` et `torch.pow`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5faddad",
   "metadata": {},
   "source": [
    "## Fonction de loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c32c82",
   "metadata": {},
   "source": [
    "Écrivez deux fonctions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "821ad69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v:torch.Tensor, t:torch.Tensor)->torch.Tensor:\n",
    "    # Implementation of ||t-v||_2^2\n",
    "    return torch.sum(torch.pow(t - v, 2.0))\n",
    "\n",
    "def dloss(v:torch.Tensor, t:torch.Tensor)->torch.Tensor:\n",
    "    # Implementation of the gradient of the loss function\n",
    "    return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4dd1b",
   "metadata": {},
   "source": [
    "rq: ${||v-t||}_2^2 = \\sqrt{\\sum{(t_i - v_i)²}}^2 = \\sum{(t_i - v_i)²}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0da4c8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "v = torch.empty(4, 3).normal_().requires_grad_(True)\n",
    "t = torch.empty(4, 3).normal_()\n",
    "l = loss(v, t) # has a grad of ones\n",
    "l.backward()\n",
    "\n",
    "print((dloss(v.detach(), t) == v.grad).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f952ac",
   "metadata": {},
   "source": [
    "qui prennent en entrée deux tenseurs de nombres à virgule flottante de mêmes dimensions, avec `v` le tenseur prédit et `t` le tenseur cible, et renvoient respectivement  $∣∣t−v∣∣_2^2$, et un tenseur de même taille égal au gradient de cette quantité en fonction de v.\n",
    "\n",
    "**Indication** : Les fonctions ne doivent pas avoir de boucle python, et utiliser en particulier `torch.sum`, `torch.pow`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75937786",
   "metadata": {},
   "source": [
    "## Passe forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7834f4",
   "metadata": {},
   "source": [
    "Écrivez une fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "db439a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(xIn:torch.Tensor, w:torch.Tensor, b:torch.Tensor)->tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"compute a step of forward pass, returns: (s, x)\"\"\"\n",
    "    s = xIn @ w.T + b\n",
    "    x = sigma(s)\n",
    "    return (s, x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22fd508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(w1:torch.Tensor, b1:torch.Tensor, w2:torch.Tensor, \n",
    "                 b2:torch.Tensor, x:torch.Tensor)->tuple[torch.Tensor, ...]:\n",
    "    \"\"\"compute the forward pass, returns: (s1, x1, s2, x2)\"\"\"\n",
    "    s1, x1 = forward_step(x, w1, b1)\n",
    "    s2, x2 = forward_step(x1, w2, b2)\n",
    "    return (s1, x1, s2, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c5671",
   "metadata": {},
   "source": [
    "dont les arguments correspondent à un vecteur d'entrée du réseau, et aux poids et biais des deux couches, et qui renvoie un tuple composé de tous les tenseurs intermédiaires le long de la passe avant :\n",
    "$$\\begin{array}{lll}\n",
    "x_0 & = & x \\\\\n",
    "s_1 & = & w_1 \\cdot x_0 + b_1 \\\\\n",
    "x_1 & = & \\sigma(s_1) \\\\\n",
    "s_2 & = & w_2 \\cdot x_1 + b_2 \\\\\n",
    "x_2 & = & \\sigma(s_2)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bbe6e",
   "metadata": {},
   "source": [
    "## Passe arrière"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429becc2",
   "metadata": {},
   "source": [
    "Écrivez une fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "35ffbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(xIn:torch.Tensor, w:torch.Tensor, \n",
    "                  s:torch.Tensor, dl_xOut:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"compute a step of the backward pass, returns: (dl_w, dl_b, dl_xIn)\"\"\"\n",
    "    dl_s = dsigma(s.detach()) * dl_xOut\n",
    "    dl_w = dl_s.T @ xIn.detach()\n",
    "    dl_b = dl_s.sum(dim=0)\n",
    "    dl_xIn = dl_s @ w.detach()\n",
    "    return (dl_w, dl_b, dl_xIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25a0c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_xOut: tensor(True)\n",
      "dl_w: tensor(True)\n",
      "dl_b: tensor(True)\n",
      "dl_xIn: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "batchSize=4; nbIn=5; nbOut = 3\n",
    "xIn = torch.empty((batchSize, nbIn)).normal_().requires_grad_(True)\n",
    "w = torch.empty((nbOut, nbIn)).normal_().requires_grad_(True)\n",
    "b = torch.empty((nbOut, )).normal_().requires_grad_(True)\n",
    "\n",
    "s = xIn @ w.T + b; s.retain_grad()\n",
    "xOut = sigma(s); xOut.retain_grad()\n",
    "l = torch.sum(xOut); l.retain_grad()\n",
    "l.backward()\n",
    "\n",
    "dl_xOut = torch.ones((batchSize, nbOut)) # cause use sum loss\n",
    "dl_w, dl_b, dl_xIn = backward_step(xIn, w, s, dl_xOut)\n",
    "\n",
    "print(\"dl_xOut:\", (dl_xOut == xOut.grad).all())\n",
    "print(\"dl_w:\", (dl_w == w.grad).all())\n",
    "print(\"dl_b:\", (dl_b == b.grad).all())\n",
    "print(\"dl_xIn:\", (dl_xIn == xIn.grad).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "93716ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(w1, b1, w2, b2, # paramètres du MLP\n",
    "                  t, # cible\n",
    "                  x, s1, x1, s2, x2, # tenseurs de la passe avant\n",
    "                  dl_dw1, dl_db1, dl_dw2, dl_db2 # gradients de la loss selon les paramètres w1, b1, w2, b2 \n",
    "                 ):\n",
    "    # compute the gradient\n",
    "    grad_x2 = dloss(v=x2, t=t)\n",
    "    grad_w2, grad_b2, grad_x1 = backward_step(x1, w2, s2, grad_x2)\n",
    "    grad_w1, grad_b1, grad_x = backward_step(x, w1, s1, grad_x1)\n",
    "    # add the gradient\n",
    "    dl_dw1 += grad_w1; dl_db1 += grad_b1\n",
    "    dl_dw2 += grad_w2; dl_db2 += grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c6a7c",
   "metadata": {},
   "source": [
    "dont les arguments correspondent aux paramètres du réseau, au vecteur cible, aux quantités calculées par la passe avant, et aux tenseurs représentant les gradients, et qui met à jour ces derniers selon la formule de la passe arrière : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f083a52",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x_2} = \\partial \\mathcal{L}(x_2, t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a380b",
   "metadata": {},
   "source": [
    "* Dérivations à travers les activations, pour le niveau $\\ell$ :\n",
    "    \n",
    "On a $x_{\\ell}(i) = \\sigma(s_{\\ell}(i))$, donc :\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}(i)} = \\sigma'(s_{\\ell}(i)) \\cdot \\frac{\\partial \\mathcal{L}}{\\partial x_{\\ell}(i)}\n",
    "$$\n",
    "\n",
    "On a $s_{\\ell}(i) = \\sum_j w_{\\ell}(i,j) \\cdot x_{\\ell - 1}(j) + b_{\\ell}(i)$, donc :\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x_{\\ell - 1}(j)} = \\sum_i w_{\\ell}(i,j) \\cdot \\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf3099",
   "metadata": {},
   "source": [
    "En notations matricielles : \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}} = \\sigma'(s_{\\ell}) \\cdot \\frac{\\partial \\mathcal{L}}{\\partial x_{\\ell}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x_{\\ell - 1}} = w_{\\ell}^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae34655",
   "metadata": {},
   "source": [
    "* Dérivations à travers les couches linéaires : \n",
    "\n",
    "On a $s_\\ell(i) = \\sum_j w_{\\ell}(i,j) \\cdot x_{\\ell - 1}(j) + b_{\\ell}(i)$, donc :\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{\\ell}(i,j)} = \\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}(i)} \\cdot x_{\\ell - 1}(j)\n",
    "$$\n",
    "et\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_{\\ell}(i)} = \\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1d973",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{\\ell}} = \\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}} \\cdot x_{\\ell - 1}^T\n",
    "$$\n",
    "et\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_{\\ell}} = \\frac{\\partial \\mathcal{L}}{\\partial s_{\\ell}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda37118",
   "metadata": {},
   "source": [
    "**Indication** : Les fonctions ne doivent pas avoir de boucle python, et utiliser en particulier `.T` (transpose), `torch.view`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efb981",
   "metadata": {},
   "source": [
    "## Boucle d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d27f4",
   "metadata": {},
   "source": [
    "Complétez le code ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "289e5480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_digits: np.ndarray; y_digits: np.ndarray\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "print(X_digits.shape, y_digits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d2f6dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 10)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(y, num_classes=10):\n",
    "    num_samples = len(y)\n",
    "    one_hot = np.zeros((num_samples, num_classes))\n",
    "    one_hot[np.arange(num_samples), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_digits_one_hot = one_hot_encode(y_digits, num_classes=10)\n",
    "print(y_digits_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5ebf05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, test_input, train_target, test_target = \\\n",
    "    train_test_split(X_digits, y_digits_one_hot, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "19af4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.tensor(train_input, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target, dtype=torch.long) \n",
    "test_input = torch.tensor(test_input, dtype=torch.float32)\n",
    "test_target = torch.tensor(test_target, dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "806f2e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1437, 64]) torch.Size([1437, 10])\n",
      "torch.Size([360, 64]) torch.Size([360, 10])\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, train_target.shape)\n",
    "print(test_input.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "800351b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_classes=10, nb_train_samples=1437\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples, nb_classes = train_target.shape\n",
    "print(f\"{nb_classes=}, {nb_train_samples=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8190f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = 0.9\n",
    "\n",
    "train_target = train_target * zeta\n",
    "test_target = test_target * zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "45fedfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta=6.96e-06\n"
     ]
    }
   ],
   "source": [
    "nb_hidden = 50\n",
    "eta = 1e-2 / nb_train_samples\n",
    "print(f\"{eta=:_.3g}\")\n",
    "epsilon = 1e-6\n",
    "\n",
    "w1 = torch.empty(nb_hidden, train_input.size(1)).normal_(0, epsilon)\n",
    "b1 = torch.empty(nb_hidden).normal_(0, epsilon)\n",
    "w2 = torch.empty(nb_classes, nb_hidden).normal_(0, epsilon)\n",
    "b2 = torch.empty(nb_classes).normal_(0, epsilon)\n",
    "\n",
    "dl_dw1 = torch.empty(w1.size())\n",
    "dl_db1 = torch.empty(b1.size())\n",
    "dl_dw2 = torch.empty(w2.size())\n",
    "dl_db2 = torch.empty(b2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "286d9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_preds: 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0\n",
      "epoch:   0, trainLoss:0.81000, tainAcc:9.5%, testAcc:7.8%\n",
      "\n",
      "test_preds: 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0\n",
      "epoch:  20, trainLoss:0.76502, tainAcc:10.7%, testAcc:7.8%\n",
      "\n",
      "test_preds: 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0\n",
      "epoch:  40, trainLoss:0.74400, tainAcc:10.7%, testAcc:7.8%\n",
      "\n",
      "test_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "epoch:  60, trainLoss:0.72937, tainAcc:10.6%, testAcc:8.3%\n",
      "\n",
      "test_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "epoch:  80, trainLoss:0.72873, tainAcc:10.6%, testAcc:8.3%\n",
      "\n",
      "test_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "epoch: 100, trainLoss:0.72843, tainAcc:10.6%, testAcc:8.3%\n",
      "\n",
      "test_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
      "epoch: 120, trainLoss:0.72682, tainAcc:10.6%, testAcc:8.3%\n",
      "\n",
      "test_preds: 0.0, 51.9, 0.0, 16.4, 0.3, 6.4, 25.0, 0.0, 0.0, 0.0\n",
      "train_preds: 0.0, 56.0, 0.0, 11.9, 0.5, 6.0, 25.6, 0.0, 0.0, 0.0\n",
      "epoch: 140, trainLoss:0.71306, tainAcc:27.3%, testAcc:26.4%\n",
      "\n",
      "test_preds: 10.0, 15.6, 0.0, 43.3, 12.8, 1.4, 11.7, 2.8, 0.0, 2.5\n",
      "train_preds: 9.1, 18.0, 0.0, 40.6, 14.5, 1.0, 12.6, 2.2, 0.0, 2.0\n",
      "epoch: 160, trainLoss:0.65280, tainAcc:49.6%, testAcc:49.7%\n",
      "\n",
      "test_preds: 13.3, 15.6, 1.7, 32.2, 11.4, 0.0, 9.4, 11.7, 0.0, 4.7\n",
      "train_preds: 12.8, 17.8, 1.9, 27.4, 12.5, 0.1, 11.5, 10.2, 0.0, 5.8\n",
      "epoch: 180, trainLoss:0.59270, tainAcc:61.0%, testAcc:61.9%\n",
      "\n",
      "test_preds: 15.0, 14.4, 7.2, 23.1, 9.4, 0.0, 9.4, 16.7, 0.0, 4.7\n",
      "train_preds: 14.0, 16.4, 6.4, 18.2, 10.8, 0.2, 11.8, 15.2, 0.0, 7.0\n",
      "epoch: 200, trainLoss:0.55688, tainAcc:65.0%, testAcc:66.1%\n",
      "\n",
      "test_preds: 14.4, 13.1, 12.2, 17.5, 9.2, 1.7, 9.4, 16.9, 0.0, 5.6\n",
      "train_preds: 13.9, 15.1, 9.4, 13.2, 10.2, 1.1, 11.9, 16.5, 0.0, 8.7\n",
      "epoch: 220, trainLoss:0.53762, tainAcc:68.9%, testAcc:71.7%\n",
      "\n",
      "test_preds: 13.6, 13.9, 14.4, 13.3, 9.2, 4.7, 9.4, 15.3, 0.0, 6.1\n",
      "train_preds: 12.3, 15.4, 10.2, 11.0, 10.2, 6.6, 11.9, 13.8, 0.1, 8.6\n",
      "epoch: 240, trainLoss:0.51410, tainAcc:75.6%, testAcc:79.2%\n",
      "\n",
      "test_preds: 11.9, 13.3, 14.4, 12.2, 9.4, 8.6, 9.4, 13.1, 0.3, 7.2\n",
      "train_preds: 10.2, 16.2, 9.8, 9.8, 10.1, 11.1, 11.8, 12.2, 0.1, 8.7\n",
      "epoch: 260, trainLoss:0.47813, tainAcc:81.4%, testAcc:83.6%\n",
      "\n",
      "test_preds: 11.7, 13.6, 13.1, 11.9, 9.4, 10.0, 8.9, 12.5, 0.8, 8.1\n",
      "train_preds: 10.0, 16.1, 9.7, 9.0, 10.0, 12.7, 11.4, 11.4, 0.4, 9.2\n",
      "epoch: 280, trainLoss:0.43800, tainAcc:83.8%, testAcc:85.3%\n",
      "\n",
      "test_preds: 11.1, 13.3, 12.5, 11.7, 9.2, 10.8, 9.2, 12.5, 0.8, 8.9\n",
      "train_preds: 10.0, 15.4, 9.7, 9.2, 10.1, 13.0, 11.2, 10.9, 1.3, 9.3\n",
      "epoch: 300, trainLoss:0.40243, tainAcc:85.7%, testAcc:86.1%\n",
      "\n",
      "test_preds: 11.4, 13.1, 12.5, 11.9, 9.2, 10.6, 9.4, 12.2, 1.7, 8.1\n",
      "train_preds: 10.0, 14.2, 9.7, 9.5, 10.2, 12.8, 11.1, 10.6, 2.4, 9.5\n",
      "epoch: 320, trainLoss:0.37081, tainAcc:87.8%, testAcc:86.7%\n",
      "\n",
      "test_preds: 11.4, 11.7, 12.5, 11.1, 9.2, 9.7, 9.4, 12.2, 5.0, 7.8\n",
      "train_preds: 10.0, 13.6, 9.7, 9.7, 10.1, 12.3, 11.1, 10.6, 3.7, 9.3\n",
      "epoch: 340, trainLoss:0.34266, tainAcc:89.4%, testAcc:90.0%\n",
      "\n",
      "test_preds: 11.4, 10.8, 12.5, 11.1, 9.2, 9.4, 9.4, 11.9, 6.1, 8.1\n",
      "train_preds: 9.9, 12.7, 9.5, 9.7, 10.2, 12.0, 11.0, 10.5, 5.7, 8.9\n",
      "epoch: 360, trainLoss:0.31774, tainAcc:91.8%, testAcc:91.7%\n",
      "\n",
      "test_preds: 11.4, 10.3, 12.5, 10.6, 9.2, 9.2, 9.4, 11.7, 7.5, 8.3\n",
      "train_preds: 9.8, 12.2, 9.5, 9.7, 10.1, 11.9, 10.6, 10.3, 6.8, 9.0\n",
      "epoch: 380, trainLoss:0.29529, tainAcc:92.9%, testAcc:92.8%\n",
      "\n",
      "test_preds: 11.4, 9.4, 12.5, 10.8, 9.2, 8.6, 9.2, 11.7, 8.6, 8.6\n",
      "train_preds: 9.7, 11.8, 9.5, 9.5, 10.1, 11.8, 10.6, 10.3, 7.4, 9.3\n",
      "epoch: 400, trainLoss:0.27479, tainAcc:93.7%, testAcc:93.9%\n",
      "\n",
      "test_preds: 11.4, 9.4, 12.5, 10.8, 9.2, 8.3, 8.9, 11.7, 8.9, 8.9\n",
      "train_preds: 9.7, 11.7, 9.5, 9.4, 10.2, 11.6, 10.4, 10.3, 7.9, 9.3\n",
      "epoch: 420, trainLoss:0.25615, tainAcc:94.3%, testAcc:95.0%\n",
      "\n",
      "test_preds: 11.4, 9.4, 12.5, 10.8, 9.2, 8.3, 8.9, 11.7, 8.9, 8.9\n",
      "train_preds: 9.7, 11.6, 9.6, 9.4, 10.2, 11.3, 10.3, 10.2, 8.4, 9.4\n",
      "epoch: 440, trainLoss:0.23949, tainAcc:94.9%, testAcc:95.0%\n",
      "\n",
      "test_preds: 11.4, 9.4, 12.5, 10.8, 9.2, 8.3, 8.9, 11.7, 8.9, 8.9\n",
      "train_preds: 9.7, 11.3, 9.7, 9.4, 10.2, 11.3, 10.2, 10.0, 8.7, 9.6\n",
      "epoch: 460, trainLoss:0.22486, tainAcc:95.6%, testAcc:95.0%\n",
      "\n",
      "test_preds: 11.1, 8.9, 12.5, 11.1, 9.2, 8.1, 8.9, 11.7, 9.4, 9.2\n",
      "train_preds: 9.6, 11.3, 9.7, 9.6, 10.2, 11.1, 10.2, 9.7, 8.8, 9.8\n",
      "epoch: 480, trainLoss:0.21212, tainAcc:96.3%, testAcc:95.6%\n",
      "\n",
      "test_preds: 11.1, 8.6, 12.5, 11.1, 9.2, 8.1, 8.9, 11.7, 9.7, 9.2\n",
      "train_preds: 9.6, 11.3, 9.6, 9.7, 10.2, 11.1, 10.2, 9.7, 8.8, 9.9\n",
      "epoch: 499, trainLoss:0.20152, tainAcc:96.4%, testAcc:95.8%\n"
     ]
    }
   ],
   "source": [
    "from itertools import batched\n",
    "N_EPOCHS = 500\n",
    "f = lambda d, tt: \", \".join(f\"{100*v/tt:.1f}\" for v in d.values())\n",
    "\n",
    "for k in range(N_EPOCHS):\n",
    "    acc_loss = 0\n",
    "    nb_train_correct = 0\n",
    "\n",
    "    # Empty the gradients here\n",
    "    dl_dw1.zero_()\n",
    "    dl_db1.zero_()\n",
    "    dl_dw2.zero_()\n",
    "    dl_db2.zero_()\n",
    "\n",
    "    train_preds: dict[int, int] = {i:0 for i in range(nb_classes)}\n",
    "    train_truths: dict[int, int] = {i:0 for i in range(nb_classes)}\n",
    "    #for n_ in map(list, batched(range(nb_train_samples), n=200)):\n",
    "    for n_ in [list(range(nb_train_samples))]:\n",
    "        # Forward pass here\n",
    "        x = train_input[n_]\n",
    "        s1, x1, s2, x2 = forward_pass(w1, b1, w2, b2, x)\n",
    "\n",
    "        # Computing the loss (only for logging purposes)\n",
    "        preds = x2.argmax(dim=1)\n",
    "        truths = train_target[n_].argmax(dim=1)\n",
    "        nb_train_correct += (truths == preds).sum().item()\n",
    "        for pred in preds.tolist(): train_preds[pred] += 1\n",
    "        for truth in truths.tolist(): train_truths[truth] += 1\n",
    "        acc_loss = acc_loss + loss(x2, train_target[n_]).sum()\n",
    "\n",
    "        # Backward pass here\n",
    "        t = train_target[n_]\n",
    "        backward_pass(w1, b1, w2, b2, t, x, s1, x1, s2, x2, dl_dw1, dl_db1, dl_dw2, dl_db2)\n",
    "    acc_loss /= train_input.size(0)\n",
    "\n",
    "    # Gradient step\n",
    "    w1 = w1 - eta * dl_dw1; b1 = b1 - eta * dl_db1\n",
    "    w2 = w2 - eta * dl_dw2; b2 = b2 - eta * dl_db2\n",
    "\n",
    "    # Test error\n",
    "    if ((k % 20 == 0) or (k == N_EPOCHS-1)): \n",
    "        nb_test_correct = 0\n",
    "        test_preds: dict[int, int] = {i:0 for i in range(nb_classes)}\n",
    "        test_truths: dict[int, int] = {i:0 for i in range(nb_classes)}\n",
    "        #for n_ in map(list, batched(range(test_input.size(0)), n=200)):\n",
    "        for n_ in [list(range(test_input.size(0)))]:\n",
    "            _, _, _, x2 = forward_pass(w1, b1, w2, b2, test_input[n_])\n",
    "\n",
    "            preds = x2.argmax(dim=1)\n",
    "            truths = test_target[n_].argmax(dim=1)\n",
    "            nb_test_correct += (truths == preds).sum().item()\n",
    "            for pred in preds.tolist(): test_preds[pred] += 1\n",
    "            for truth in truths.tolist(): test_truths[truth] += 1\n",
    "\n",
    "        trainAccuracy: float = nb_train_correct / train_input.size(0)\n",
    "        testAccuracy: float = nb_test_correct / test_input.size(0)\n",
    "        print()\n",
    "        print(\"test_preds:\", f(test_preds, test_input.size(0)))\n",
    "        #print(\"test_truths:\", f(test_truths, test_input.size(0)))\n",
    "        #print(\"----\")\n",
    "        print(\"train_preds:\", f(train_preds, train_input.size(0)))\n",
    "        #print(\"train_truths:\", f(train_truths, train_input.size(0)))\n",
    "        print(f\"epoch:{k:>4d}, trainLoss:{acc_loss:.5f}, tainAcc:{trainAccuracy:.1%}, testAcc:{testAccuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4995e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
